#!/usr/bin/env python3
"""
ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•è„šæœ¬

æµ‹è¯•ç›®æ ‡:
1. éªŒè¯ä¸åŒé˜ˆå€¼ (0.6, 0.65, 0.7, 0.75, 0.8) çš„æ•ˆæœ
2. å¯¹æ¯”ä¸åŒé”™è¯¯ç±»å‹æ˜¯å¦éœ€è¦ä¸åŒé˜ˆå€¼
3. æ‰¾å‡ºæœ€ä¼˜é˜ˆå€¼é…ç½®

æµ‹è¯•æŒ‡æ ‡:
- å¿«é€Ÿè·¯å¾„å‘½ä¸­ç‡: ä½¿ç”¨å¿«é€Ÿè·¯å¾„ä¿®å¤çš„æ¯”ä¾‹
- æˆåŠŸç‡: ä¿®å¤æˆåŠŸçš„æ¯”ä¾‹
- LLMè°ƒç”¨æ¬¡æ•°: å¹³å‡æ¯ä¸ªç”¨ä¾‹çš„LLMè°ƒç”¨æ¬¡æ•°
- å¹³å‡è€—æ—¶: æ¯ä¸ªç”¨ä¾‹çš„å¹³å‡å¤„ç†æ—¶é—´
"""

import sys
import os
import json
import time
import subprocess
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Optional
import asyncio

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.core.error_identifier import ErrorIdentifier
from src.strategies.registry import ErrorStrategyRegistry
from src.tools_new.context_tools import ContextTools


class ConfidenceThresholdTester:
    """ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•å™¨"""

    def __init__(self, test_cases_dir: str = "tests/test_cases_v2"):
        self.test_cases_dir = Path(test_cases_dir)
        self.error_identifier = ErrorIdentifier()
        self.results = []

    def load_test_cases(self) -> Dict[str, List[Path]]:
        """åŠ è½½æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ï¼ŒæŒ‰é”™è¯¯ç±»å‹åˆ†ç±»"""
        test_cases = defaultdict(list)

        for error_type_dir in self.test_cases_dir.iterdir():
            if not error_type_dir.is_dir() or error_type_dir.name.startswith('.'):
                continue

            error_type = error_type_dir.name

            for case_dir in error_type_dir.iterdir():
                if case_dir.is_dir() and case_dir.name.startswith('case_'):
                    main_file = case_dir / "main.py"
                    if main_file.exists():
                        test_cases[error_type].append(case_dir)

        return test_cases

    def run_test_case(self, case_dir: Path) -> Optional[Dict]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œè·å–é”™è¯¯ä¿¡æ¯"""
        main_file = case_dir / "main.py"

        # è¯»å–å…ƒæ•°æ®
        metadata_file = case_dir / "metadata.json"
        metadata = {}
        if metadata_file.exists():
            with open(metadata_file) as f:
                metadata = json.load(f)

        # è¿è¡Œæµ‹è¯•è·å–é”™è¯¯
        try:
            result = subprocess.run(
                [sys.executable, str(main_file)],
                cwd=str(case_dir),
                capture_output=True,
                text=True,
                timeout=5
            )

            if result.returncode != 0:
                # è¯†åˆ«é”™è¯¯
                error = self.error_identifier.identify(result.stderr)

                return {
                    'case_id': metadata.get('case_id', case_dir.name),
                    'case_dir': str(case_dir),
                    'error_type': metadata.get('error_type', error.error_type),
                    'difficulty': metadata.get('difficulty', 'unknown'),
                    'error': error,
                    'traceback': result.stderr,
                    'metadata': metadata
                }
        except Exception as e:
            print(f"âš ï¸  Error running {case_dir.name}: {e}")

        return None

    def test_strategy_confidence(
        self,
        error_type: str,
        test_case: Dict,
        threshold: float
    ) -> Dict:
        """æµ‹è¯•å•ä¸ªç”¨ä¾‹åœ¨æŒ‡å®šé˜ˆå€¼ä¸‹çš„è¡¨ç°"""

        # åˆ›å»ºç­–ç•¥æ³¨å†Œè¡¨
        registry = ErrorStrategyRegistry()
        registry.register_all_defaults(confidence_threshold=threshold)

        # è·å–å¯¹åº”çš„ç­–ç•¥
        strategy = registry.get(error_type)
        if not strategy:
            return {
                'used_fast_path': False,
                'confidence': 0.0,
                'reason': 'No strategy found'
            }

        # æå–é”™è¯¯ä¿¡æ¯
        error = test_case['error']
        extracted = strategy.extract(error.error_message)

        # åˆ›å»º ContextToolsï¼ˆéœ€è¦é¡¹ç›®è·¯å¾„ï¼‰
        case_dir = Path(test_case['case_dir'])
        try:
            context_tools = ContextTools(str(case_dir))

            # æ‰§è¡Œå¿«é€Ÿæœç´¢
            search_result = strategy.fast_search(
                extracted,
                context_tools,
                error.error_file
            )

            if search_result and search_result.confidence >= threshold:
                return {
                    'used_fast_path': True,
                    'confidence': search_result.confidence,
                    'found': search_result.symbol or search_result.location,
                    'reason': 'Fast path - confidence above threshold'
                }
            else:
                return {
                    'used_fast_path': False,
                    'confidence': search_result.confidence if search_result else 0.0,
                    'reason': f'Confidence {search_result.confidence if search_result else 0.0:.2f} below threshold {threshold}'
                }
        except Exception as e:
            return {
                'used_fast_path': False,
                'confidence': 0.0,
                'reason': f'Error: {str(e)[:100]}'
            }

    def test_threshold(self, threshold: float, test_cases: Dict[str, List[Path]]) -> Dict:
        """æµ‹è¯•æŒ‡å®šé˜ˆå€¼ä¸‹çš„æ•´ä½“è¡¨ç°"""
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•é˜ˆå€¼: {threshold}")
        print(f"{'='*60}")

        results_by_type = defaultdict(list)

        for error_type, cases in test_cases.items():
            print(f"\n{error_type}: {len(cases)} ä¸ªç”¨ä¾‹")

            for case_dir in cases:
                # è¿è¡Œæµ‹è¯•ç”¨ä¾‹
                test_case = self.run_test_case(case_dir)
                if not test_case:
                    continue

                # æµ‹è¯•ç­–ç•¥ç½®ä¿¡åº¦
                result = self.test_strategy_confidence(
                    error_type,
                    test_case,
                    threshold
                )

                result['case_id'] = test_case['case_id']
                result['difficulty'] = test_case['difficulty']
                results_by_type[error_type].append(result)

                # æ˜¾ç¤ºè¿›åº¦
                status = "âœ… å¿«é€Ÿ" if result['used_fast_path'] else "ğŸ” è°ƒæŸ¥"
                conf = result['confidence']
                print(f"  {status} {test_case['case_id'][:40]:40s} ç½®ä¿¡åº¦: {conf:.3f}")

        # ç»Ÿè®¡ç»“æœ
        return self.calculate_statistics(threshold, results_by_type)

    def calculate_statistics(self, threshold: float, results_by_type: Dict) -> Dict:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        stats = {
            'threshold': threshold,
            'by_type': {},
            'overall': {}
        }

        total_cases = 0
        total_fast_path = 0
        all_confidences = []

        for error_type, results in results_by_type.items():
            fast_path_count = sum(1 for r in results if r['used_fast_path'])
            confidences = [r['confidence'] for r in results]

            stats['by_type'][error_type] = {
                'total': len(results),
                'fast_path_count': fast_path_count,
                'fast_path_rate': fast_path_count / len(results) if results else 0,
                'avg_confidence': sum(confidences) / len(confidences) if confidences else 0,
                'max_confidence': max(confidences) if confidences else 0,
                'min_confidence': min(confidences) if confidences else 0
            }

            total_cases += len(results)
            total_fast_path += fast_path_count
            all_confidences.extend(confidences)

        stats['overall'] = {
            'total': total_cases,
            'fast_path_count': total_fast_path,
            'fast_path_rate': total_fast_path / total_cases if total_cases else 0,
            'avg_confidence': sum(all_confidences) / len(all_confidences) if all_confidences else 0
        }

        return stats

    def run_all_tests(self, thresholds: List[float]) -> List[Dict]:
        """è¿è¡Œæ‰€æœ‰é˜ˆå€¼çš„æµ‹è¯•"""
        print("\n" + "="*60)
        print("ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•")
        print("="*60)

        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        test_cases = self.load_test_cases()

        print(f"\nåŠ è½½æµ‹è¯•ç”¨ä¾‹:")
        total = 0
        for error_type, cases in test_cases.items():
            print(f"  {error_type:20s} {len(cases)} ä¸ª")
            total += len(cases)
        print(f"  {'æ€»è®¡':20s} {total} ä¸ª")

        # å¯¹æ¯ä¸ªé˜ˆå€¼è¿›è¡Œæµ‹è¯•
        all_results = []
        for threshold in thresholds:
            result = self.test_threshold(threshold, test_cases)
            all_results.append(result)
            self.results.append(result)

        return all_results

    def generate_report(self, results: List[Dict]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("\n" + "="*80)
        report.append("ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•æŠ¥å‘Š")
        report.append("="*80)

        # 1. æ•´ä½“å¯¹æ¯”
        report.append("\n## 1. ä¸åŒé˜ˆå€¼çš„æ•´ä½“è¡¨ç°")
        report.append("\n| é˜ˆå€¼ | å¿«é€Ÿè·¯å¾„å‘½ä¸­ç‡ | å¹³å‡ç½®ä¿¡åº¦ | æ€»ç”¨ä¾‹æ•° |")
        report.append("|------|---------------|-----------|---------|")

        for result in results:
            threshold = result['threshold']
            overall = result['overall']
            report.append(
                f"| {threshold:.2f} | "
                f"{overall['fast_path_rate']*100:.1f}% | "
                f"{overall['avg_confidence']:.3f} | "
                f"{overall['total']} |"
            )

        # 2. æŒ‰é”™è¯¯ç±»å‹å¯¹æ¯”
        report.append("\n## 2. ä¸åŒé”™è¯¯ç±»å‹çš„é˜ˆå€¼éœ€æ±‚")

        # è·å–æ‰€æœ‰é”™è¯¯ç±»å‹
        error_types = set()
        for result in results:
            error_types.update(result['by_type'].keys())

        for error_type in sorted(error_types):
            report.append(f"\n### {error_type}")
            report.append("\n| é˜ˆå€¼ | å¿«é€Ÿè·¯å¾„å‘½ä¸­ç‡ | å¹³å‡ç½®ä¿¡åº¦ | ç”¨ä¾‹æ•° |")
            report.append("|------|---------------|-----------|--------|")

            for result in results:
                if error_type in result['by_type']:
                    stats = result['by_type'][error_type]
                    report.append(
                        f"| {result['threshold']:.2f} | "
                        f"{stats['fast_path_rate']*100:.1f}% | "
                        f"{stats['avg_confidence']:.3f} | "
                        f"{stats['total']} |"
                    )

        # 3. å»ºè®®
        report.append("\n## 3. å»ºè®®çš„é˜ˆå€¼é…ç½®")
        report.append("\nåŸºäºæµ‹è¯•ç»“æœçš„å»ºè®®:")

        # æ‰¾å‡ºæœ€ä¼˜é˜ˆå€¼
        best_overall = max(results, key=lambda r: r['overall']['fast_path_rate'])
        report.append(f"\n**æ•´ä½“æœ€ä¼˜é˜ˆå€¼**: {best_overall['threshold']:.2f}")
        report.append(f"  - å¿«é€Ÿè·¯å¾„å‘½ä¸­ç‡: {best_overall['overall']['fast_path_rate']*100:.1f}%")
        report.append(f"  - å¹³å‡ç½®ä¿¡åº¦: {best_overall['overall']['avg_confidence']:.3f}")

        # æŒ‰é”™è¯¯ç±»å‹æ¨è
        report.append("\n**æŒ‰é”™è¯¯ç±»å‹æ¨è**:")
        for error_type in sorted(error_types):
            type_results = [
                (r['threshold'], r['by_type'][error_type]['fast_path_rate'])
                for r in results if error_type in r['by_type']
            ]
            best_threshold, best_rate = max(type_results, key=lambda x: x[1])
            report.append(f"  - {error_type:20s} {best_threshold:.2f} (å‘½ä¸­ç‡ {best_rate*100:.1f}%)")

        return "\n".join(report)

    def save_results(self, filename: str = "confidence_test_results.json"):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        with open(filename, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {filename}")


def main():
    """ä¸»å‡½æ•°"""
    # æµ‹è¯•çš„é˜ˆå€¼åˆ—è¡¨
    thresholds = [0.60, 0.65, 0.70, 0.75, 0.80]

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ConfidenceThresholdTester()

    # è¿è¡Œæµ‹è¯•
    results = tester.run_all_tests(thresholds)

    # ç”ŸæˆæŠ¥å‘Š
    report = tester.generate_report(results)
    print(report)

    # ä¿å­˜ç»“æœ
    tester.save_results()

    # åŒæ—¶ä¿å­˜æŠ¥å‘Š
    with open("confidence_test_report.md", 'w') as f:
        f.write(report)
    print("âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: confidence_test_report.md")


if __name__ == "__main__":
    main()
