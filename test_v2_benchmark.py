#!/usr/bin/env python3
"""V2 Benchmark æ‰¹é‡æµ‹è¯•è„šæœ¬ - æ”¯æŒ MiMo vs Claude å¯¹æ¯”"""
import sys
import os
import json
import time
import subprocess
from pathlib import Path
from datetime import datetime
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent))

from src.core.error_identifier import ErrorIdentifier
from src.strategies.registry import ErrorStrategyRegistry
from src.tools_new.context_tools import ContextTools


class V2BenchmarkTester:
    """V2 Benchmark æµ‹è¯•å™¨"""

    def __init__(self, test_cases_dir: str = "tests/test_cases_v2"):
        self.test_cases_dir = Path(test_cases_dir)
        self.results = []

        # æ£€æµ‹å½“å‰ä½¿ç”¨çš„æ¨¡å‹
        base_url = os.getenv('ANTHROPIC_BASE_URL', 'Anthropic Official API')
        self.model_name = 'mimo' if 'mimo' in base_url.lower() else 'claude'

        print(f"\n{'='*70}")
        print(f"V2 Benchmark æµ‹è¯•")
        print(f"{'='*70}")
        print(f"æ¨¡å‹: {self.model_name}")
        print(f"API: {base_url}")
        print(f"{'='*70}\n")

    def load_test_cases(self, limit: int = None) -> list:
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        for error_type_dir in sorted(self.test_cases_dir.iterdir()):
            if not error_type_dir.is_dir() or error_type_dir.name.startswith('.'):
                continue

            error_type = error_type_dir.name

            for case_dir in sorted(error_type_dir.iterdir()):
                if case_dir.is_dir() and (case_dir / "main.py").exists():
                    test_cases.append({
                        'path': case_dir,
                        'error_type': error_type
                    })

                    if limit and len(test_cases) >= limit:
                        return test_cases

        return test_cases

    def test_single_case(self, case_info: dict, index: int, total: int) -> dict:
        """æµ‹è¯•å•ä¸ªç”¨ä¾‹"""
        case_dir = case_info['path']
        error_type = case_info['error_type']

        # è¯»å– metadata
        metadata_file = case_dir / "metadata.json"
        metadata = {}
        if metadata_file.exists():
            with open(metadata_file) as f:
                metadata = json.load(f)

        case_id = metadata.get('case_id', case_dir.name)
        difficulty = metadata.get('difficulty', 'unknown')

        print(f"\n{'='*70}")
        print(f"[{index}/{total}] {case_id}")
        print(f"ç±»å‹: {error_type} | éš¾åº¦: {difficulty}")
        print(f"{'='*70}")

        # è¿è¡Œæµ‹è¯•è·å–é”™è¯¯
        try:
            result = subprocess.run(
                [sys.executable, "main.py"],
                cwd=str(case_dir),
                capture_output=True,
                text=True,
                timeout=10
            )
        except subprocess.TimeoutExpired:
            print(f"â±ï¸  è¶…æ—¶ï¼ˆ10ç§’ï¼‰")
            return {
                'case_id': case_id,
                'error_type': error_type,
                'difficulty': difficulty,
                'success': False,
                'reason': 'Timeout',
                'duration': 10
            }

        if result.returncode == 0:
            print(f"âš ï¸  ç¨‹åºæœ¬èº«æ²¡æœ‰é”™è¯¯ï¼Œè·³è¿‡")
            return {
                'case_id': case_id,
                'error_type': error_type,
                'difficulty': difficulty,
                'success': False,
                'reason': 'No error found',
                'duration': 0,
                'skipped': True
            }

        print(f"ğŸ” æ£€æµ‹åˆ°é”™è¯¯ï¼Œå¼€å§‹ä¿®å¤...")

        # ä½¿ç”¨ç­–ç•¥å°è¯•å¿«é€Ÿä¿®å¤
        start_time = time.time()
        try:
            # è¯†åˆ«é”™è¯¯
            identifier = ErrorIdentifier()
            error = identifier.identify(result.stderr)

            print(f"   é”™è¯¯ç±»å‹: {error.error_type}")
            print(f"   é”™è¯¯æ–‡ä»¶: {error.error_file}")

            # è·å–ç­–ç•¥
            registry = ErrorStrategyRegistry()
            registry.register_all_defaults()
            strategy = registry.get(error.error_type)

            if not strategy:
                duration = time.time() - start_time
                print(f"âŒ æ— å¯¹åº”ç­–ç•¥")
                return {
                    'case_id': case_id,
                    'error_type': error_type,
                    'difficulty': difficulty,
                    'success': False,
                    'reason': 'No strategy',
                    'duration': duration
                }

            # æå–é”™è¯¯ä¿¡æ¯
            extracted = strategy.extract(error.error_message)
            print(f"   æå–ä¿¡æ¯: {extracted}")

            # åˆ›å»º ContextTools
            context_tools = ContextTools(str(case_dir))

            # å¿«é€Ÿæœç´¢
            search_result = strategy.fast_search(extracted, context_tools, error.error_file)

            if search_result and search_result.confidence >= 0.7:
                print(f"   âœ… å¿«é€Ÿè·¯å¾„å‘½ä¸­ (ç½®ä¿¡åº¦: {search_result.confidence:.2f})")
                # è¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…åº”è¯¥è°ƒç”¨ä¿®å¤é€»è¾‘
                # ä½†ä¸ºäº†æµ‹è¯•ï¼Œæˆ‘ä»¬åªè®°å½•æ˜¯å¦æ‰¾åˆ°äº†é«˜ç½®ä¿¡åº¦åŒ¹é…
                success = True
            else:
                conf = search_result.confidence if search_result else 0.0
                print(f"   ğŸ” éœ€è¦å®Œæ•´æ¢ç´¢ (ç½®ä¿¡åº¦: {conf:.2f})")
                # éœ€è¦ ReAct å®Œæ•´æ¢ç´¢
                success = False

            duration = time.time() - start_time

            result_dict = {
                'case_id': case_id,
                'error_type': error_type,
                'difficulty': difficulty,
                'success': success,
                'duration': duration,
                'confidence': search_result.confidence if search_result else 0.0,
                'used_fast_path': success
            }

            if success:
                print(f"âœ… æµ‹è¯•å®Œæˆ (è€—æ—¶: {duration:.1f}s)")
            else:
                print(f"âš ï¸  éœ€è¦å®Œæ•´æ¢ç´¢ (è€—æ—¶: {duration:.1f}s)")

            return result_dict

        except Exception as e:
            duration = time.time() - start_time
            print(f"âŒ å¼‚å¸¸: {str(e)[:100]}")

            return {
                'case_id': case_id,
                'error_type': error_type,
                'difficulty': difficulty,
                'success': False,
                'duration': duration,
                'error': str(e)[:200]
            }

    def run_batch_test(self, limit: int = None):
        """æ‰¹é‡æµ‹è¯•"""
        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        test_cases = self.load_test_cases(limit=limit)

        print(f"åŠ è½½ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        if limit:
            print(f"(é™åˆ¶: {limit} ä¸ª)")

        # è¿è¡Œæµ‹è¯•
        for i, case_info in enumerate(test_cases, 1):
            result = self.test_single_case(case_info, i, len(test_cases))

            if not result.get('skipped'):
                self.results.append(result)

            # æ˜¾ç¤ºå½“å‰ç»Ÿè®¡
            if self.results:
                success_count = sum(1 for r in self.results if r.get('success'))
                print(f"\nğŸ“Š å½“å‰ç»Ÿè®¡: {success_count}/{len(self.results)} æˆåŠŸ ({success_count/len(self.results)*100:.1f}%)")

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()

    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n\n" + "="*70)
        print("æµ‹è¯•æŠ¥å‘Š")
        print("="*70)

        if not self.results:
            print("\nâš ï¸  æ— æœ‰æ•ˆæµ‹è¯•ç»“æœ")
            return

        # æ•´ä½“ç»Ÿè®¡
        total = len(self.results)
        success_count = sum(1 for r in self.results if r.get('success'))
        success_rate = success_count / total * 100

        durations = [r.get('duration', 0) for r in self.results]
        avg_duration = sum(durations) / len(durations) if durations else 0

        confidences = [r.get('confidence', 0) for r in self.results]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0

        print(f"\n## æ•´ä½“è¡¨ç°")
        print(f"   æ€»ç”¨ä¾‹æ•°: {total}")
        print(f"   æˆåŠŸæ•°: {success_count}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"   å¹³å‡è€—æ—¶: {avg_duration:.1f}ç§’")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")

        # æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡
        by_type = defaultdict(lambda: {'total': 0, 'success': 0, 'durations': [], 'confidences': []})
        for r in self.results:
            error_type = r.get('error_type', 'unknown')
            by_type[error_type]['total'] += 1
            if r.get('success'):
                by_type[error_type]['success'] += 1
            by_type[error_type]['durations'].append(r.get('duration', 0))
            by_type[error_type]['confidences'].append(r.get('confidence', 0))

        print(f"\n## æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡")
        print(f"{'ç±»å‹':<20s} {'æˆåŠŸç‡':<20s} {'å¹³å‡è€—æ—¶':<12s} {'å¹³å‡ç½®ä¿¡åº¦'}")
        print("-" * 70)
        for error_type, stats in sorted(by_type.items()):
            rate = stats['success'] / stats['total'] * 100
            avg_dur = sum(stats['durations']) / len(stats['durations'])
            avg_conf = sum(stats['confidences']) / len(stats['confidences'])
            print(f"{error_type:<20s} {stats['success']}/{stats['total']} ({rate:>5.1f}%) {avg_dur:>10.1f}s {avg_conf:>12.3f}")

        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = f"v2_test_{self.model_name}_{timestamp}.json"

        with open(result_file, 'w') as f:
            json.dump({
                'timestamp': timestamp,
                'model': self.model_name,
                'base_url': os.getenv('ANTHROPIC_BASE_URL', 'Anthropic Official'),
                'total': total,
                'success_count': success_count,
                'success_rate': success_rate,
                'avg_duration': avg_duration,
                'avg_confidence': avg_confidence,
                'by_type': dict(by_type),
                'results': self.results
            }, f, indent=2)

        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        return result_file


def main():
    import argparse

    parser = argparse.ArgumentParser(description='V2 Benchmark æ‰¹é‡æµ‹è¯•')
    parser.add_argument('--limit', type=int, help='é™åˆ¶æµ‹è¯•ç”¨ä¾‹æ•°é‡')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•ï¼ˆ6ä¸ªç”¨ä¾‹ï¼‰')

    args = parser.parse_args()

    limit = 6 if args.quick else args.limit

    tester = V2BenchmarkTester()
    tester.run_batch_test(limit=limit)


if __name__ == "__main__":
    main()
