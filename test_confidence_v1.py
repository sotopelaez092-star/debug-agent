#!/usr/bin/env python3
"""V1 ç®€å•ç”¨ä¾‹ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•"""
import sys
import os
import json
import subprocess
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Optional

sys.path.insert(0, str(Path(__file__).parent))

from src.core.error_identifier import ErrorIdentifier
from src.strategies.registry import ErrorStrategyRegistry
from src.tools_new.context_tools import ContextTools


class V1ConfidenceThresholdTester:
    """V1 ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•å™¨"""

    def __init__(self, test_cases_dir: str = "tests/test_cases_v1"):
        self.test_cases_dir = Path(test_cases_dir)
        self.error_identifier = ErrorIdentifier()
        self.results = []

    def load_test_cases(self) -> List[Path]:
        """åŠ è½½æ‰€æœ‰ V1 æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        for case_dir in self.test_cases_dir.iterdir():
            if case_dir.is_dir() and not case_dir.name.startswith('.'):
                main_file = case_dir / "main.py"
                if main_file.exists():
                    test_cases.append(case_dir)

        return sorted(test_cases)

    def run_test_case(self, case_dir: Path) -> Optional[Dict]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        main_file = case_dir / "main.py"

        # è¯»å–å…ƒæ•°æ®
        metadata_file = case_dir / "metadata.json"
        metadata = {}
        if metadata_file.exists():
            with open(metadata_file) as f:
                metadata = json.load(f)

        # è¿è¡Œæµ‹è¯•è·å–é”™è¯¯
        try:
            result = subprocess.run(
                [sys.executable, "main.py"],  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
                cwd=str(case_dir),
                capture_output=True,
                text=True,
                timeout=5
            )

            if result.returncode != 0:
                # è¯†åˆ«é”™è¯¯
                error = self.error_identifier.identify(result.stderr)

                return {
                    'case_id': metadata.get('case_id', case_dir.name),
                    'case_dir': str(case_dir),
                    'error_type': error.error_type,
                    'error': error,
                    'traceback': result.stderr,
                    'metadata': metadata
                }
        except Exception as e:
            print(f"âš ï¸  Error running {case_dir.name}: {e}")

        return None

    def test_strategy_confidence(
        self,
        test_case: Dict,
        threshold: float
    ) -> Dict:
        """æµ‹è¯•å•ä¸ªç”¨ä¾‹åœ¨æŒ‡å®šé˜ˆå€¼ä¸‹çš„è¡¨ç°"""

        # åˆ›å»ºç­–ç•¥æ³¨å†Œè¡¨
        registry = ErrorStrategyRegistry()
        registry.register_all_defaults(confidence_threshold=threshold)

        # è·å–å¯¹åº”çš„ç­–ç•¥
        error_type = test_case['error_type']
        strategy = registry.get(error_type)
        if not strategy:
            return {
                'used_fast_path': False,
                'confidence': 0.0,
                'reason': 'No strategy found'
            }

        # æå–é”™è¯¯ä¿¡æ¯
        error = test_case['error']
        extracted = strategy.extract(error.error_message)

        # åˆ›å»º ContextTools
        case_dir = Path(test_case['case_dir'])
        try:
            context_tools = ContextTools(str(case_dir))

            # æ‰§è¡Œå¿«é€Ÿæœç´¢
            search_result = strategy.fast_search(
                extracted,
                context_tools,
                error.error_file
            )

            if search_result and search_result.confidence >= threshold:
                return {
                    'used_fast_path': True,
                    'confidence': search_result.confidence,
                    'found': search_result.symbol or search_result.location,
                    'reason': 'Fast path - confidence above threshold'
                }
            else:
                return {
                    'used_fast_path': False,
                    'confidence': search_result.confidence if search_result else 0.0,
                    'reason': f'Confidence {search_result.confidence if search_result else 0.0:.2f} below threshold {threshold}',
                    'search_result': search_result
                }
        except Exception as e:
            import traceback
            return {
                'used_fast_path': False,
                'confidence': 0.0,
                'reason': f'Error: {str(e)}',
                'traceback': traceback.format_exc()
            }

    def test_threshold(self, threshold: float, test_cases: List[Path]) -> Dict:
        """æµ‹è¯•æŒ‡å®šé˜ˆå€¼ä¸‹çš„æ•´ä½“è¡¨ç°"""
        print(f"\n{'='*70}")
        print(f"æµ‹è¯•é˜ˆå€¼: {threshold}")
        print(f"{'='*70}")

        results = []

        for case_dir in test_cases:
            # è¿è¡Œæµ‹è¯•ç”¨ä¾‹
            test_case = self.run_test_case(case_dir)
            if not test_case:
                continue

            # æµ‹è¯•ç­–ç•¥ç½®ä¿¡åº¦
            result = self.test_strategy_confidence(test_case, threshold)

            result['case_id'] = test_case['case_id']
            result['error_type'] = test_case['error_type']
            result['edit_distance'] = test_case['metadata'].get('edit_distance', 0)
            result['expected_similarity'] = test_case['metadata'].get('expected_similarity', 0)
            results.append(result)

            # æ˜¾ç¤ºè¿›åº¦
            status = "âœ… å¿«é€Ÿ" if result['used_fast_path'] else "ğŸ” æ¢ç´¢"
            conf = result['confidence']
            edit_dist = result['edit_distance']
            reason = result.get('reason', '')
            print(f"  {status} [{edit_dist}] {test_case['case_id']:45s} ç½®ä¿¡åº¦: {conf:.3f}")
            if conf == 0.0 and 'traceback' in result:
                print(f"      ERROR: {result['reason']}")
                print(f"      {result['traceback'][:200]}")

        # ç»Ÿè®¡ç»“æœ
        return self.calculate_statistics(threshold, results)

    def calculate_statistics(self, threshold: float, results: List[Dict]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        stats = {
            'threshold': threshold,
            'results': results,
            'overall': {}
        }

        total_cases = len(results)
        total_fast_path = sum(1 for r in results if r['used_fast_path'])
        confidences = [r['confidence'] for r in results]

        # æŒ‰ç¼–è¾‘è·ç¦»åˆ†ç»„ç»Ÿè®¡
        by_edit_distance = defaultdict(lambda: {'total': 0, 'fast_path': 0, 'confidences': []})
        for r in results:
            ed = r['edit_distance']
            by_edit_distance[ed]['total'] += 1
            if r['used_fast_path']:
                by_edit_distance[ed]['fast_path'] += 1
            by_edit_distance[ed]['confidences'].append(r['confidence'])

        stats['overall'] = {
            'total': total_cases,
            'fast_path_count': total_fast_path,
            'fast_path_rate': total_fast_path / total_cases if total_cases else 0,
            'avg_confidence': sum(confidences) / len(confidences) if confidences else 0
        }

        stats['by_edit_distance'] = {}
        for ed, data in by_edit_distance.items():
            stats['by_edit_distance'][ed] = {
                'total': data['total'],
                'fast_path_count': data['fast_path'],
                'fast_path_rate': data['fast_path'] / data['total'] if data['total'] else 0,
                'avg_confidence': sum(data['confidences']) / len(data['confidences']) if data['confidences'] else 0
            }

        return stats

    def run_all_tests(self, thresholds: List[float]) -> List[Dict]:
        """è¿è¡Œæ‰€æœ‰é˜ˆå€¼çš„æµ‹è¯•"""
        print("\n" + "="*70)
        print("V1 ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•")
        print("="*70)

        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        test_cases = self.load_test_cases()

        print(f"\nåŠ è½½ V1 æµ‹è¯•ç”¨ä¾‹: {len(test_cases)} ä¸ª")

        # å¯¹æ¯ä¸ªé˜ˆå€¼è¿›è¡Œæµ‹è¯•
        all_results = []
        for threshold in thresholds:
            result = self.test_threshold(threshold, test_cases)
            all_results.append(result)
            self.results.append(result)

        return all_results

    def generate_report(self, results: List[Dict]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("\n" + "="*80)
        report.append("V1 ç½®ä¿¡åº¦é˜ˆå€¼æµ‹è¯•æŠ¥å‘Š")
        report.append("="*80)

        # æ•´ä½“å¯¹æ¯”
        report.append("\n## ä¸åŒé˜ˆå€¼çš„æ•´ä½“è¡¨ç°")
        report.append("\n| é˜ˆå€¼ | å¿«é€Ÿè·¯å¾„å‘½ä¸­ç‡ | å¹³å‡ç½®ä¿¡åº¦ | æ€»ç”¨ä¾‹æ•° |")
        report.append("|------|---------------|-----------|---------|")

        for result in results:
            threshold = result['threshold']
            overall = result['overall']
            report.append(
                f"| {threshold:.2f} | "
                f"{overall['fast_path_rate']*100:.1f}% | "
                f"{overall['avg_confidence']:.3f} | "
                f"{overall['total']} |"
            )

        # æŒ‰ç¼–è¾‘è·ç¦»åˆ†æ
        report.append("\n## ä¸åŒç¼–è¾‘è·ç¦»çš„é˜ˆå€¼æ•ˆæœ")

        # æ”¶é›†æ‰€æœ‰ç¼–è¾‘è·ç¦»
        all_edit_distances = set()
        for result in results:
            all_edit_distances.update(result.get('by_edit_distance', {}).keys())

        for edit_dist in sorted(all_edit_distances):
            report.append(f"\n### ç¼–è¾‘è·ç¦» = {edit_dist}")
            report.append("\n| é˜ˆå€¼ | å¿«é€Ÿè·¯å¾„å‘½ä¸­ç‡ | å¹³å‡ç½®ä¿¡åº¦ | ç”¨ä¾‹æ•° |")
            report.append("|------|---------------|-----------|--------|")

            for result in results:
                if edit_dist in result.get('by_edit_distance', {}):
                    stats = result['by_edit_distance'][edit_dist]
                    report.append(
                        f"| {result['threshold']:.2f} | "
                        f"{stats['fast_path_rate']*100:.1f}% | "
                        f"{stats['avg_confidence']:.3f} | "
                        f"{stats['total']} |"
                    )

        # è¯¦ç»†ç”¨ä¾‹ç»“æœ
        report.append("\n## è¯¦ç»†ç”¨ä¾‹ç»“æœ")
        report.append("\n| ç”¨ä¾‹ | ç¼–è¾‘è·ç¦» | é˜ˆå€¼ 0.6 | é˜ˆå€¼ 0.7 | é˜ˆå€¼ 0.75 | é˜ˆå€¼ 0.8 |")
        report.append("|------|---------|---------|---------|----------|---------|")

        # æ”¶é›†æ¯ä¸ªç”¨ä¾‹åœ¨ä¸åŒé˜ˆå€¼ä¸‹çš„ç½®ä¿¡åº¦
        case_results = defaultdict(dict)
        for result in results:
            threshold = result['threshold']
            for case_result in result.get('results', []):
                case_id = case_result['case_id']
                case_results[case_id]['edit_distance'] = case_result['edit_distance']
                case_results[case_id][threshold] = case_result['confidence']

        for case_id in sorted(case_results.keys()):
            data = case_results[case_id]
            edit_dist = data.get('edit_distance', 0)
            report.append(
                f"| {case_id[:40]} | {edit_dist} | "
                f"{data.get(0.60, 0):.3f} | "
                f"{data.get(0.70, 0):.3f} | "
                f"{data.get(0.75, 0):.3f} | "
                f"{data.get(0.80, 0):.3f} |"
            )

        # å»ºè®®
        report.append("\n## å»ºè®®")

        best_threshold = max(results, key=lambda r: r['overall']['fast_path_rate'])
        report.append(f"\n**æœ€ä¼˜é˜ˆå€¼**: {best_threshold['threshold']:.2f}")
        report.append(f"  - å¿«é€Ÿè·¯å¾„å‘½ä¸­ç‡: {best_threshold['overall']['fast_path_rate']*100:.1f}%")
        report.append(f"  - å¹³å‡ç½®ä¿¡åº¦: {best_threshold['overall']['avg_confidence']:.3f}")

        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    thresholds = [0.60, 0.65, 0.70, 0.75, 0.80]

    tester = V1ConfidenceThresholdTester()
    results = tester.run_all_tests(thresholds)

    # ç”ŸæˆæŠ¥å‘Š
    report = tester.generate_report(results)
    print(report)

    # ä¿å­˜ç»“æœ
    with open("confidence_test_v1_results.json", 'w') as f:
        json.dump(tester.results, f, indent=2, default=str)
    print("\nâœ… ç»“æœå·²ä¿å­˜åˆ°: confidence_test_v1_results.json")

    # ä¿å­˜æŠ¥å‘Š
    with open("confidence_test_v1_report.md", 'w') as f:
        f.write(report)
    print("âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: confidence_test_v1_report.md")


if __name__ == "__main__":
    main()
