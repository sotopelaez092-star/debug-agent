# src/rag/retriever.py
"""æ–‡æ¡£æ£€ç´¢å™¨"""
from typing import List, Dict, Any, Optional, final
import logging
from chromadb import Collection

logger = logging.getLogger(__name__)

class BaseRetriever:
    """
    åŸºç¡€æ£€ç´¢å™¨

    åŠŸèƒ½ï¼š
    1. æ ¹æ®é”™è¯¯ä¿¡æ¯æ£€ç´¢ç›¸å…³çš„è§£å†³æ–¹æ¡ˆ
    2. è¿‡æ»¤ä½ç›¸å…³åº¦ç»“æœ
    3. æ ¼å¼åŒ–è¾“å‡º
    """


    def __init__(
        self,
        collection: Collection,
        min_similarity: float = 0.5,
        recall_factor: int = 4
    ):
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        if not collection:
            raise ValueError('collectionä¸èƒ½ä¸ºç©º')
        if not isinstance(min_similarity, (int, float)):
            raise ValueError('min_similarityå¿…é¡»æ˜¯æ•°å­—')
        
        # âœ… æ”¹è¿™é‡Œï¼šå…è®¸è´Ÿæ•°
        if min_similarity < -1 or min_similarity > 1:
            raise ValueError('min_similarityå¿…é¡»åœ¨-1åˆ°1ä¹‹é—´')  # æ”¹æˆ -1 åˆ° 1
        
        if not isinstance(recall_factor, int):
            raise TypeError('recall_factorå¿…é¡»æ˜¯æ•´æ•°')
        if recall_factor < 1:
            raise ValueError('recall_factorå¿…é¡»å¤§äºç­‰äº1')
        
        self.collection = collection
        self.min_similarity = min_similarity
        self.recall_factor = recall_factor
        
        logger.info(
            f"åˆå§‹åŒ–BaseRetriever: min_similarity={min_similarity},"
            f"recall_factor={recall_factor}"
        )

    def search(
        self,
        query: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        f"""
        æ£€ç´¢ç›¸å…³æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆé”™è¯¯ä¿¡æ¯ï¼‰
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼Œæ ¼å¼ï¼š
            [
                {{
                    "id": æ–‡æ¡£ID,
                    "content": æ–‡æ¡£å†…å®¹,
                    'similarity': ç›¸ä¼¼åº¦åˆ†æ•°
                    "metadata": å…ƒæ•°æ®å­—å…¸,
                    "rank": æ’å
                }}
                ...
            ]
        Raises:
            ValueError: å½“queryä¸ºç©ºæˆ–top_kä¸åˆæ³•æ—¶
        """

        # 1. è¾“å…¥éªŒè¯
        if not query or not isinstance(query, str):
            raise ValueError('queryå¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²')
        if len(query) > 10000:
            logger.warning(f"queryè¿‡é•¿({len(query)}å­—ç¬¦)ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        if not isinstance(top_k, int):
            raise TypeError('top_kå¿…é¡»æ˜¯æ•´æ•°')
        if top_k < 1:
            raise ValueError('top_kå¿…é¡»æ˜¯æ­£æ•´æ•°')
        if top_k > 100:
            raise ValueError('top_kä¸èƒ½è¶…è¿‡100')

        logger.info(f"å¼€å§‹æ£€ç´¢ï¼Œqueryé•¿åº¦={len(query)}ï¼Œtop_k={top_k}")

        # 2. æŸ¥è¯¢é¢„å¤„ç†
        cleaned_query = self._preprocess_query(query)
        logger.debug(f"é¢„å¤„ç†åçš„query: {cleaned_query}")

        # 3. å‘é‡æ£€ç´¢ï¼ˆå¬å› top_k * recall_factor ä¸ªå€™é€‰ï¼‰
        n_results = top_k * self.recall_factor
        raw_results = self._vector_search(cleaned_query, n_results)
        
        # 4. è¿‡æ»¤ä½ç›¸å…³åº¦
        filtered_results = self._filter_by_similarity(
            raw_results,
            self.min_similarity
        )

        # 5. æ ¼å¼åŒ–è¾“å‡º
        final_results = self._format_results(filtered_results, top_k)

        logger.info(f"æ ¼å¼åŒ–å®Œæˆï¼Œè¿”å›{len(final_results)}ä¸ªç»“æœ")
        return final_results

    def _preprocess_query(self, query: str) -> str:
        """
        æ¸…ç†æŸ¥è¯¢æ–‡æœ¬ï¼Œæå–å…³é”®ä¿¡æ¯

        å¤„ç†æ­¥éª¤ï¼š
        1. å»é™¤Tracebackè¡Œ
        2. å»é™¤æ–‡ä»¶è·¯å¾„ä¿¡æ¯ï¼ˆFile â€œxxxâ€ï¼Œ line xxxï¼‰
        3. æå–é”™è¯¯ç±»å‹å’Œæ¶ˆæ¯
        4. é™åˆ¶é•¿åº¦ï¼ˆé¿å…è¶…è¿‡embeddingæ¨¡å‹tokené™åˆ¶)

        Args:
            query: åŸå§‹é”™è¯¯ä¿¡æ¯

        Returns:
            æ¸…ç†åçš„æŸ¥è¯¢æ–‡æœ¬
        """
        # 1. æŒ‰è¡Œåˆ†å‰²
        lines = query.split('\n')

        # 2. è¿‡æ»¤æ— ç”¨è¡Œ
        cleaned_lines = []
        for line in lines:
            if line.strip().startswith('Traceback'):
                continue
            if line.strip().startswith('File'):
                continue
            if not line.strip():
                continue
            cleaned_lines.append(line.strip())

        MAX_LENGTH = 500
        # 3. é‡æ–°ç»„åˆ
        cleaned = '\n'.join(cleaned_lines)

        # 4. ä¿æŒé•¿åº¦
        if len(cleaned) > MAX_LENGTH:
            logger.warning(f"æŸ¥è¯¢æ–‡æœ¬è¿‡é•¿({len(cleaned)}å­—ç¬¦)ï¼Œå·²æˆªæ–­ä¸º{MAX_LENGTH}å­—ç¬¦")
            cleaned = cleaned[:MAX_LENGTH]

        return cleaned

    def _vector_search(
        self,
        query: str,
        n_results: int
    ) -> Dict[str, List]:
        """
        å‘é‡æœç´¢

        Args:
            query: æ¸…ç†åçš„æŸ¥è¯¢æ–‡æ¡£
            n_results: æ‰¾å›æ–‡æ¡£æ•°é‡ (top_k * recall_Factor)

        Returns:
            ChromaDBåŸå§‹è¿”å›ç»“æœ
            æ ¼å¼ï¼š
            {
                "id": [['id1', 'id2', ...]],
                "document": [['doc1', 'doc2', ...]],
                "metadata": [[{...}, {...}, {...}]],
                'distance': [[0.2, 0.3, ...]]
            }

        Raises:
            Exception: å½“æ£€ç´¢å¤±è´¥æ—¶
        """
        try:
            logger.debug(f"å¼€å§‹å‘é‡æ£€ç´¢ï¼Œn_results={n_results}")
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                include=['documents', 'metadatas', 'distances']
            )

            num_results = len(results['ids'][0]) if results['ids'] else 0
            logger.info(f"æ£€ç´¢å®Œæˆï¼Œå¬å›{num_results}ä¸ªæ–‡æ¡£")

            return results          
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}", exc_info=True)
            raise

    def _filter_by_similarity(
        self,
        raw_results: Dict[str, List],
        min_similarity: float
    ) -> Dict[str, List]:
        """
        è¿‡æ»¤ä½ç›¸å…³åº¦çš„ç»“æœ

        Args:
            raw_results: ChromaDBåŸå§‹è¿”å›ç»“æœ
            min_similarity: æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            è¿‡æ»¤åçš„ç»“æœï¼ˆä»ç„¶æ˜¯ChromaDBæ ¼å¼ï¼‰
        """
        # 1. å–å‡ºç¬¬ä¸€æ‰¹æ¬¡çš„æ•°æ®ï¼ˆå› ä¸ºæ˜¯åµŒå¥—åˆ—è¡¨ï¼‰
        ids = raw_results['ids'][0] if raw_results['ids'] else []
        documents = raw_results['documents'][0] if raw_results['documents'] else []
        metadatas = raw_results['metadatas'][0] if raw_results['metadatas'] else []
        distances = raw_results['distances'][0] if raw_results['distances'] else []

        print("\nğŸ” è°ƒè¯•ä¿¡æ¯ - ç›¸ä¼¼åº¦åˆ†æ•°ï¼š")
        for i, (id, dist) in enumerate(zip(ids[:5], distances[:5])):  # åªçœ‹å‰5ä¸ª
            similarity = 1 - dist
            print(f"  {i+1}. ID={id}: distance={dist:.4f}, similarity={similarity:.4f}")
        print()
        # 2. è¿‡æ»¤
        filtered_ids = []
        filtered_documents = []
        filtered_metadatas = []
        filtered_distances = []

        for id, doc, meta, dist in zip(ids, documents, metadatas, distances):
            similarity = 1- dist

            if similarity >= min_similarity:
                filtered_ids.append(id)
                filtered_documents.append(doc)
                filtered_metadatas.append(meta)
                filtered_distances.append(dist)

        # 3. æ—¥å¿—
        logger.info(
            f"è¿‡æ»¤å®Œæˆï¼š{len(ids)} -> {len(filtered_ids)}"
            f"ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š{min_similarity}"
        )

        # 4. ç»„ç»‡è¿”å›ç»“æœ
        filtered_results = {
            'ids': [filtered_ids],
            'documents': [filtered_documents],
            'metadatas': [filtered_metadatas],
            'distances': [filtered_distances]
        }

        return filtered_results

    def _format_results(
        self,
        raw_results: Dict[str, List],
        top_k: int
    ) -> List[Dict[str, Any]]:
        """
        æ ¼å¼åŒ–æ£€ç´¢ç»“æœ

        Args:
            raw_results: ChromaDBåŸå§‹è¿”å›ç»“æœï¼ˆå·²è¿‡æ»¤ï¼‰
            top_k: æœ€ç»ˆè¿”å›æ•°é‡

        Returns:
            æ ¼å¼åŒ–çš„ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        # 1. å–å‡ºç¬¬ä¸€æ‰¹æ¬¡çš„æ•°æ®ï¼ˆå› ä¸ºæ˜¯åµŒå¥—åˆ—è¡¨ï¼‰
        ids = raw_results['ids'][0] if raw_results['ids'] else []
        documents = raw_results['documents'][0] if raw_results['documents'] else []
        metadatas = raw_results['metadatas'][0] if raw_results['metadatas'] else []
        distances = raw_results['distances'][0] if raw_results['distances'] else []

        # 2. è½¬æ¢æ ¼å¼
        formatted_results = []
        for id, doc, meta, dist in zip(ids, documents, metadatas, distances):
            formatted_results.append({
                'id': id,
                'content': doc,
                'metadata': meta,
                'similarity': 1 - dist,
                'distance': dist
            })

        # 3. æ’åº
        formatted_results.sort(key=lambda x: x['similarity'], reverse=True)

        # 4. é™åˆ¶æ•°é‡ + æ·»åŠ rank
        final_results = []
        for rank, result in enumerate(formatted_results[:top_k], start=1):
            result['rank'] = rank
            final_results.append(result)

        logger.info(f"æ ¼å¼åŒ–å®Œæˆï¼Œè¿”å›{len(final_results)}ä¸ªç»“æœ")

        return final_results
