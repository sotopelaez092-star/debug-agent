# src/rag/embedder.py
"""Embeddingç”Ÿæˆå™¨"""
from typing import List, Dict
import numpy as np
from sentence_transformers import SentenceTransformer

# ============ å…¨å±€å•ä¾‹ ============
_global_embedder_instance = None

def get_embedder_instance(model_name: str = "BAAI/bge-small-en-v1.5") -> 'Embedder':
    """
    è·å–å…¨å±€Embedderå•ä¾‹
    
    Args:
        model_name: æ¨¡å‹åç§°
        
    Returns:
        Embedderå®ä¾‹ï¼ˆå…¨å±€å”¯ä¸€ï¼‰
    """
    global _global_embedder_instance
    
    if _global_embedder_instance is None:
        print(f"ğŸ”§ é¦–æ¬¡åˆ›å»ºEmbedderå•ä¾‹...")
        _global_embedder_instance = Embedder(model_name)
    else:
        print(f"âœ… å¤ç”¨å·²æœ‰çš„Embedderå®ä¾‹")
    
    return _global_embedder_instance
class Embedder:
    """åˆå§‹åŒ–Embeddingç”Ÿæˆå™¨"""

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """åˆå§‹åŒ–Embedder
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ç”¨è½»é‡çº§æ¨¡å‹ï¼‰
        """
        print(f"ğŸ“¦ åŠ è½½Embeddingæ¨¡å‹: {model_name}")

        # åŠ è½½æ¨¡å‹
        self.model = SentenceTransformer(model_name)

        # è·å–å‘é‡ç»´åº¦
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å‘é‡ç»´åº¦: {self.embedding_dim}")

    def encode_text(self, text: str) -> np.ndarray:
        """æŠŠä¸€æ®µæ–‡æœ¬è½¬æ¢æˆå‘é‡

        Args:
            text: è¾“å…¥æ–‡æœ¬ï¼Œæ¯”å¦‚ "How to reverse a list?"

        Returns:
            å‘é‡ (å‘é‡ï¼ˆnumpyæ•°ç»„ï¼‰ï¼Œæ¯”å¦‚ [0.2, -0.5, 0.8, ...])
        """
        return self.model.encode(text, show_progress_bar=False)

    def encode_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """æ‰¹é‡ç¼–ç å¤šä¸ªæ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯”å¦‚ ["text1", "text2", "text3"]
            batch_size: æ¯æ‰¹å¤„ç†å¤šå°‘ä¸ªï¼ˆ32æ˜¯å¹³è¡¡é€Ÿåº¦å’Œå†…å­˜çš„å¥½é€‰æ‹©ï¼‰
            
        Returns:
            å‘é‡çŸ©é˜µï¼Œshape=(æ–‡æœ¬æ•°é‡, 384)
        """
        print(f"ğŸ”„ å¼€å§‹ç¼–ç  {len(texts)} æ¡æ–‡æœ¬...")
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True
        )
        
        print(f"âœ… ç¼–ç å®Œæˆ")
        return embeddings

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        LangChainå…¼å®¹æ¥å£ï¼šæ‰¹é‡ç¼–ç æ–‡æ¡£
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨ï¼ˆæ¯ä¸ªå‘é‡æ˜¯floatåˆ—è¡¨ï¼‰
        """
        # ä½¿ç”¨ç°æœ‰çš„ encode_batch æ–¹æ³•
        embeddings = self.encode_batch(texts)
        
        # è½¬æ¢ä¸º List[List[float]] æ ¼å¼ï¼ˆBaseRetrieveræœŸæœ›çš„æ ¼å¼ï¼‰
        return embeddings.tolist()

    def process_chunks_with_embeddings(self, chunks: List[Dict]) -> List[Dict]:
        """ä¸ºæ–‡æœ¬å—ç”Ÿæˆembeddings
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨ï¼ˆæ¥è‡ªTextChunkerï¼‰
            
        Returns:
            å¸¦æœ‰embeddingçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        # æå–æ‰€æœ‰æ–‡æœ¬
        texts = [chunk['text'] for chunk in chunks]
        
        # æ‰¹é‡ç”Ÿæˆembeddings
        embeddings = self.encode_batch(texts)
        
        # å°†embeddingæ·»åŠ åˆ°æ¯ä¸ªå—
        for chunk, embedding in zip(chunks, embeddings):
            chunk['embedding'] = embedding.tolist()
        
        return chunks


