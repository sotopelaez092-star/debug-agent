# src/utils/test_llm.py
"""
æµ‹è¯•LLMè¿æ¥
éªŒè¯DeepSeek APIæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.llm_factory import get_llm
from src.utils.config import get_settings


def test_llm_basic():
    """æµ‹è¯•åŸºç¡€LLMè°ƒç”¨"""
    print("=" * 50)
    print("æµ‹è¯•1: åŸºç¡€LLMè°ƒç”¨")
    print("=" * 50)
    
    try:
        # è·å–LLM
        llm = get_llm()
        
        # ç®€å•æµ‹è¯•
        response = llm.invoke("Say 'Hello from DeepSeek!' in one sentence")
        
        print(f"âœ… å“åº”: {response.content}")
        print(f"âœ… Tokenä½¿ç”¨: {response.response_metadata}")
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False


def test_llm_chinese():
    """æµ‹è¯•ä¸­æ–‡æ”¯æŒ"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•2: ä¸­æ–‡ç†è§£")
    print("=" * 50)
    
    try:
        llm = get_llm(temperature=0.3)
        
        response = llm.invoke("ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰")
        
        print(f"âœ… å“åº”: {response.content}")
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False


def test_llm_code():
    """æµ‹è¯•ä»£ç ç”Ÿæˆ"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•3: ä»£ç ç”Ÿæˆ")
    print("=" * 50)
    
    try:
        llm = get_llm(temperature=0.1)
        
        prompt = """Write a Python function to calculate factorial. 
Only return the code, no explanation."""
        
        response = llm.invoke(prompt)
        
        print(f"âœ… å“åº”:\n{response.content}")
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False


def test_config():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•4: é…ç½®åŠ è½½")
    print("=" * 50)
    
    try:
        settings = get_settings()
        
        print(f"LLM Provider: {settings.llm_provider}")
        print(f"LLM Model: {settings.llm_model}")
        print(f"Temperature: {settings.llm_temperature}")
        print(f"Chunk Size: {settings.chunk_size}")
        
        # æ£€æŸ¥API Keyæ˜¯å¦é…ç½®
        if settings.deepseek_api_key:
            masked_key = settings.deepseek_api_key[:8] + "..." + settings.deepseek_api_key[-4:]
            print(f"âœ… DeepSeek API Key: {masked_key}")
        else:
            print("âŒ DeepSeek API Keyæœªé…ç½®ï¼")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return False


if __name__ == "__main__":
    print("\nğŸš€ å¼€å§‹æµ‹è¯•LLMè¿æ¥...\n")
    
    results = {
        "é…ç½®åŠ è½½": test_config(),
        "åŸºç¡€è°ƒç”¨": test_llm_basic(),
        "ä¸­æ–‡æ”¯æŒ": test_llm_chinese(),
        "ä»£ç ç”Ÿæˆ": test_llm_code(),
    }
    
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 50)
    
    for test_name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    all_passed = all(results.values())
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼DeepSeek APIé…ç½®æ­£ç¡®ã€‚")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
    
    sys.exit(0 if all_passed else 1)