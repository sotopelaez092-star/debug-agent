"""
PerformanceMonitoræµ‹è¯•è„šæœ¬
"""

import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.agent.performance_monitor import PerformanceMonitor


def test_basic_functionality():
    """æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½"""
    print("\n" + "="*60)
    print("æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½")
    print("="*60)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = PerformanceMonitor()
    
    # è®°å½•3æ¡æ•°æ®ï¼ˆ2æˆåŠŸï¼Œ1å¤±è´¥ï¼‰
    test_data = [
        {
            "error_type": "NameError",
            "success": True,
            "total_time": 7.5,
            "attempts": 1,
            "total_tokens": 1250,
            "prompt_tokens": 800,
            "completion_tokens": 450,
            "llm_calls": 1
        },
        {
            "error_type": "NameError",
            "success": True,
            "total_time": 6.2,
            "attempts": 1,
            "total_tokens": 1100,
            "prompt_tokens": 700,
            "completion_tokens": 400,
            "llm_calls": 1
        },
        {
            "error_type": "ImportError",
            "success": False,
            "total_time": 8.1,
            "attempts": 3,
            "total_tokens": 1400,
            "prompt_tokens": 900,
            "completion_tokens": 500,
            "llm_calls": 3
        }
    ]
    
    for data in test_data:
        monitor.record_execution(data)
    
    print(f"âœ… æˆåŠŸè®°å½• {len(test_data)} æ¡æ•°æ®")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = monitor.generate_report()
    
    # éªŒè¯æ€»ä½“ç»Ÿè®¡
    summary = report['summary']
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  - æ€»æ‰§è¡Œæ¬¡æ•°: {summary['total_executions']}")
    print(f"  - æˆåŠŸ: {summary['successful']}")
    print(f"  - å¤±è´¥: {summary['failed']}")
    print(f"  - æˆåŠŸç‡: {summary['success_rate']:.1%}")
    print(f"  - å¹³å‡è€—æ—¶: {summary['avg_time']}ç§’")
    print(f"  - æ€»Tokenæ•°: {summary['total_tokens']}")
    print(f"  - æ€»æˆæœ¬: ${summary['total_cost']:.6f}")
    print(f"  - å¹³å‡å°è¯•æ¬¡æ•°: {summary['avg_attempts']}")
    
    # æ–­è¨€éªŒè¯
    assert summary['total_executions'] == 3, "æ€»æ‰§è¡Œæ¬¡æ•°é”™è¯¯"
    assert summary['successful'] == 2, "æˆåŠŸæ¬¡æ•°é”™è¯¯"
    assert summary['failed'] == 1, "å¤±è´¥æ¬¡æ•°é”™è¯¯"
    assert abs(summary['success_rate'] - 0.667) < 0.01, "æˆåŠŸç‡é”™è¯¯"
    
    # éªŒè¯æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡
    by_error = report['by_error_type']
    print(f"\nğŸ“ˆ æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡:")
    for error_type, stats in by_error.items():
        print(f"  {error_type}:")
        print(f"    - æ•°é‡: {stats['count']}")
        print(f"    - æˆåŠŸç‡: {stats['success_rate']:.1%}")
        print(f"    - å¹³å‡è€—æ—¶: {stats['avg_time']}ç§’")
        print(f"    - å¹³å‡å°è¯•: {stats['avg_attempts']}")
    
    assert by_error['NameError']['success_rate'] == 1.0, "NameErroræˆåŠŸç‡é”™è¯¯"
    assert by_error['ImportError']['success_rate'] == 0.0, "ImportErroræˆåŠŸç‡é”™è¯¯"
    
    print("\nâœ… æµ‹è¯•1é€šè¿‡ï¼")


def test_empty_data():
    """æµ‹è¯•2: ç©ºæ•°æ®"""
    print("\n" + "="*60)
    print("æµ‹è¯•2: ç©ºæ•°æ®")
    print("="*60)
    
    monitor = PerformanceMonitor()
    report = monitor.generate_report()
    
    print(f"ç©ºæ•°æ®æŠ¥å‘Š: {report}")
    
    assert "error" in report, "åº”è¯¥è¿”å›é”™è¯¯ä¿¡æ¯"
    assert report['error'] == "æ²¡æœ‰æ‰§è¡Œè®°å½•", "é”™è¯¯ä¿¡æ¯ä¸æ­£ç¡®"
    
    print("âœ… æµ‹è¯•2é€šè¿‡ï¼")


def test_invalid_data():
    """æµ‹è¯•3: æ— æ•ˆæ•°æ®"""
    print("\n" + "="*60)
    print("æµ‹è¯•3: æ— æ•ˆæ•°æ®")
    print("="*60)
    
    monitor = PerformanceMonitor()
    
    # æµ‹è¯•1: None
    try:
        monitor.record_execution(None)
        assert False, "åº”è¯¥æŠ›å‡ºValueError"
    except ValueError as e:
        print(f"âœ… æ­£ç¡®æ‹’ç»None: {e}")
    
    # æµ‹è¯•2: ç¼ºå°‘å¿…éœ€å­—æ®µ
    try:
        monitor.record_execution({"error_type": "NameError"})
        assert False, "åº”è¯¥æŠ›å‡ºValueError"
    except ValueError as e:
        print(f"âœ… æ­£ç¡®æ‹’ç»ç¼ºå°‘å­—æ®µ: {e}")
    
    # æµ‹è¯•3: ç©ºå­—å…¸
    try:
        monitor.record_execution({})
        assert False, "åº”è¯¥æŠ›å‡ºValueError"
    except ValueError as e:
        print(f"âœ… æ­£ç¡®æ‹’ç»ç©ºå­—å…¸: {e}")
    
    print("\nâœ… æµ‹è¯•3é€šè¿‡ï¼")


def test_file_operations():
    """æµ‹è¯•4: æ–‡ä»¶ä¿å­˜å’ŒåŠ è½½"""
    print("\n" + "="*60)
    print("æµ‹è¯•4: æ–‡ä»¶ä¿å­˜å’ŒåŠ è½½")
    print("="*60)
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶è·¯å¾„
    test_file = "data/test_performance.json"
    
    # åˆ›å»ºç›‘æ§å™¨å¹¶è®°å½•æ•°æ®
    monitor1 = PerformanceMonitor()
    monitor1.record_execution({
        "error_type": "NameError",
        "success": True,
        "total_time": 5.0,
        "attempts": 1
    })
    monitor1.record_execution({
        "error_type": "TypeError",
        "success": False,
        "total_time": 7.0,
        "attempts": 2
    })
    
    print(f"è®°å½•äº† {len(monitor1.executions)} æ¡æ•°æ®")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    monitor1.save_to_file(test_file)
    print(f"âœ… ä¿å­˜åˆ°: {test_file}")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    assert os.path.exists(test_file), "æ–‡ä»¶æœªåˆ›å»º"
    
    # åŠ è½½åˆ°æ–°ç›‘æ§å™¨
    monitor2 = PerformanceMonitor()
    monitor2.load_from_file(test_file)
    print(f"âœ… ä»æ–‡ä»¶åŠ è½½äº† {len(monitor2.executions)} æ¡æ•°æ®")
    
    # éªŒè¯æ•°æ®ä¸€è‡´
    assert len(monitor2.executions) == 2, "åŠ è½½çš„æ•°æ®é‡ä¸å¯¹"
    assert monitor2.executions[0]['error_type'] == "NameError", "æ•°æ®å†…å®¹ä¸å¯¹"
    assert monitor2.executions[1]['error_type'] == "TypeError", "æ•°æ®å†…å®¹ä¸å¯¹"
    
    # ç”ŸæˆæŠ¥å‘ŠéªŒè¯
    report = monitor2.generate_report()
    assert report['summary']['total_executions'] == 2, "æŠ¥å‘Šç»Ÿè®¡ä¸å¯¹"
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    os.remove(test_file)
    print(f"âœ… æ¸…ç†æµ‹è¯•æ–‡ä»¶: {test_file}")
    
    print("\nâœ… æµ‹è¯•4é€šè¿‡ï¼")


def test_cache_mechanism():
    """æµ‹è¯•5: ç¼“å­˜æœºåˆ¶"""
    print("\n" + "="*60)
    print("æµ‹è¯•5: ç¼“å­˜æœºåˆ¶")
    print("="*60)
    
    monitor = PerformanceMonitor()
    
    # è®°å½•æ•°æ®
    monitor.record_execution({
        "error_type": "NameError",
        "success": True,
        "total_time": 5.0,
        "attempts": 1
    })
    
    # ç¬¬ä¸€æ¬¡ç”ŸæˆæŠ¥å‘Šï¼ˆè®¡ç®—ï¼‰
    report1 = monitor.generate_report()
    print("âœ… ç¬¬ä¸€æ¬¡ç”ŸæˆæŠ¥å‘Šï¼ˆè®¡ç®—ï¼‰")
    
    # ç¬¬äºŒæ¬¡ç”ŸæˆæŠ¥å‘Šï¼ˆåº”è¯¥ç”¨ç¼“å­˜ï¼‰
    report2 = monitor.generate_report()
    print("âœ… ç¬¬äºŒæ¬¡ç”ŸæˆæŠ¥å‘Šï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰")
    
    # éªŒè¯ä¸¤æ¬¡ç»“æœç›¸åŒ
    assert report1 == report2, "ç¼“å­˜ç»“æœä¸ä¸€è‡´"
    
    # è®°å½•æ–°æ•°æ®ï¼ˆåº”è¯¥æ¸…é™¤ç¼“å­˜ï¼‰
    monitor.record_execution({
        "error_type": "TypeError",
        "success": False,
        "total_time": 7.0,
        "attempts": 2
    })
    
    # å†æ¬¡ç”ŸæˆæŠ¥å‘Šï¼ˆåº”è¯¥é‡æ–°è®¡ç®—ï¼‰
    report3 = monitor.generate_report()
    print("âœ… è®°å½•æ–°æ•°æ®åé‡æ–°ç”ŸæˆæŠ¥å‘Š")
    
    # éªŒè¯ç»“æœä¸åŒ
    assert report3['summary']['total_executions'] == 2, "ç¼“å­˜æœªæ­£ç¡®æ¸…é™¤"
    
    print("\nâœ… æµ‹è¯•5é€šè¿‡ï¼")


def test_real_scenario():
    """æµ‹è¯•6: çœŸå®åœºæ™¯æ¨¡æ‹Ÿ"""
    print("\n" + "="*60)
    print("æµ‹è¯•6: çœŸå®åœºæ™¯æ¨¡æ‹Ÿï¼ˆæ¨¡æ‹ŸRouter Agentæ‰§è¡Œï¼‰")
    print("="*60)
    
    monitor = PerformanceMonitor()
    
    # æ¨¡æ‹Ÿ10æ¬¡Debugæ‰§è¡Œ
    scenarios = [
        {"error_type": "NameError", "success": True, "time": 7.5, "attempts": 1, "tokens": 1250},
        {"error_type": "NameError", "success": True, "time": 6.2, "attempts": 1, "tokens": 1100},
        {"error_type": "ImportError", "success": True, "time": 8.1, "attempts": 1, "tokens": 1400},
        {"error_type": "AttributeError", "success": True, "time": 7.8, "attempts": 1, "tokens": 1300},
        {"error_type": "TypeError", "success": False, "time": 9.5, "attempts": 3, "tokens": 2100},
        {"error_type": "NameError", "success": True, "time": 6.9, "attempts": 1, "tokens": 1150},
        {"error_type": "ValueError", "success": True, "time": 7.2, "attempts": 1, "tokens": 1200},
        {"error_type": "KeyError", "success": True, "time": 6.5, "attempts": 1, "tokens": 1050},
        {"error_type": "IndexError", "success": False, "time": 10.2, "attempts": 3, "tokens": 2300},
        {"error_type": "NameError", "success": True, "time": 7.1, "attempts": 1, "tokens": 1180},
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        data = {
            "error_type": scenario["error_type"],
            "success": scenario["success"],
            "total_time": scenario["time"],
            "attempts": scenario["attempts"],
            "total_tokens": scenario["tokens"],
            "prompt_tokens": int(scenario["tokens"] * 0.64),  # çº¦64%æ˜¯prompt
            "completion_tokens": int(scenario["tokens"] * 0.36),  # çº¦36%æ˜¯completion
            "llm_calls": scenario["attempts"]
        }
        monitor.record_execution(data)
        print(f"  [{i}/10] {scenario['error_type']}: {'âœ… æˆåŠŸ' if scenario['success'] else 'âŒ å¤±è´¥'}")
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    report = monitor.generate_report()
    
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š")
    print("="*60)
    
    summary = report['summary']
    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"  - æ€»æ‰§è¡Œæ¬¡æ•°: {summary['total_executions']}")
    print(f"  - æˆåŠŸ: {summary['successful']} ({summary['success_rate']:.1%})")
    print(f"  - å¤±è´¥: {summary['failed']}")
    print(f"  - å¹³å‡è€—æ—¶: {summary['avg_time']}ç§’")
    print(f"  - æ€»Tokenæ•°: {summary['total_tokens']:,}")
    print(f"  - æ€»æˆæœ¬: ${summary['total_cost']:.6f}")
    print(f"  - å¹³å‡å°è¯•æ¬¡æ•°: {summary['avg_attempts']}")
    
    print(f"\næŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡:")
    for error_type, stats in sorted(report['by_error_type'].items()):
        print(f"  {error_type}:")
        print(f"    æ•°é‡: {stats['count']}, "
              f"æˆåŠŸç‡: {stats['success_rate']:.0%}, "
              f"å¹³å‡è€—æ—¶: {stats['avg_time']}s, "
              f"å¹³å‡å°è¯•: {stats['avg_attempts']}")
    
    # ä¿å­˜æŠ¥å‘Š
    output_file = "data/test_report.json"
    monitor.save_to_file(output_file)
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
    
    print("\nâœ… æµ‹è¯•6é€šè¿‡ï¼")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸ§ª " + "="*58 + " ğŸ§ª")
    print("ğŸ§ª  PerformanceMonitor å®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("ğŸ§ª " + "="*58 + " ğŸ§ª")
    
    try:
        test_basic_functionality()
        test_empty_data()
        test_invalid_data()
        test_file_operations()
        test_cache_mechanism()
        test_real_scenario()
        
        print("\n" + "ğŸ‰ " + "="*58 + " ğŸ‰")
        print("ğŸ‰  æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ‰ " + "="*58 + " ğŸ‰\n")
        
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_all_tests()