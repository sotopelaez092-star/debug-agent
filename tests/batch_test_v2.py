#!/usr/bin/env python3
"""
Router vs ReAct æ‰¹é‡å¯¹æ¯”æµ‹è¯• V2
æ”¹è¿›ï¼š
1. åˆ†ç¦»å­˜å‚¨ç»“æ„ï¼ˆå°summary + è¯¦ç»†ç»“æœåˆ†å¼€ï¼‰
2. å¤±è´¥æ¡ˆä¾‹å•ç‹¬ä¿å­˜ï¼ŒåŒ…å«å®Œæ•´LLMæ€è€ƒ
3. ç”ŸæˆMarkdownå¤±è´¥æ‘˜è¦
4. æ”¯æŒ --fresh-start æ¸…é™¤æ—§æ•°æ®
"""

import sys
import os
import json
import time
import argparse
from typing import Dict, List, Any
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.agent.debug_agent import DebugAgent as RouterAgent
from src.agent.react_agent import ReActAgent
from dotenv import load_dotenv

load_dotenv()

# ============== é…ç½® ==============
NUM_RUNS = 3
MAX_WORKERS = 4

# æ”¹è¿›çš„è¾“å‡ºç»“æ„
OUTPUT_DIR = "data/evaluation/batch_comparison"
SUMMARY_FILE = f"{OUTPUT_DIR}/summary.json"
FAILURES_MD_FILE = f"{OUTPUT_DIR}/failures_summary.md"
DETAILED_DIR = f"{OUTPUT_DIR}/detailed_results"
FAILURES_DIR = f"{OUTPUT_DIR}/failures"
CHECKPOINT_FILE = f"{OUTPUT_DIR}/checkpoint.json"

# API Key
api_key = os.getenv('DEEPSEEK_API_KEY')
if not api_key:
    print("âŒ Error: DEEPSEEK_API_KEY not set")
    sys.exit(1)


# ============== æµ‹è¯•é›†åŠ è½½ï¼ˆä¸å˜ï¼‰==============

def load_all_test_cases() -> List[Dict]:
    """åŠ è½½æ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹ï¼ˆæ„é€  + çœŸå®bugï¼‰"""
    all_cases = []
    
    # 1. åŠ è½½æ„é€ æ¡ˆä¾‹
    print("ğŸ“‚ Loading constructed test cases...")
    with open('data/test_cases/week6_test_set.json', 'r', encoding='utf-8') as f:
        constructed = json.load(f)['test_cases']
    
    for case in constructed:
        all_cases.append({
            'id': f"constructed_{case['id']}",
            'name': case['name'],
            'source': 'constructed',
            'error_type': case['error_type'],
            'category': case['category'],
            'project_files': case['project_files'],
            'error_file': case['error_file'],
            'error_message': case['error_message']
        })
    
    print(f"   âœ… Loaded {len(constructed)} constructed cases")
    
    # 2. åŠ è½½çœŸå®bug
    print("ğŸ“‚ Loading BugsinPy test cases...")
    with open('data/BugsInPy-master/test_cases_info.json', 'r', encoding='utf-8') as f:
        real_bugs = json.load(f)
    
    for case in real_bugs:
        all_cases.append({
            'id': case['id'],
            'name': f"Real: {case['id']}",
            'source': 'bugsinpy',
            'project_path': case['project_path'],
            'error_file': case['error_file'],
            'undefined_name': case['undefined_name'],
            'expected_import': case['expected_import']
        })
    
    print(f"   âœ… Loaded {len(real_bugs)} BugsinPy cases")
    print(f"   ğŸ“Š Total: {len(all_cases)} cases\n")
    
    return all_cases


def prepare_test_input(case: Dict) -> tuple:
    """å‡†å¤‡æµ‹è¯•è¾“å…¥ï¼ˆç»Ÿä¸€ä¸¤ç§æ ¼å¼ï¼‰"""
    
    if case['source'] == 'constructed':
        # æ„é€ æ¡ˆä¾‹ï¼šç›´æ¥ä»project_filesè·å–
        buggy_code = list(case['project_files'].values())[0]
        error_traceback = f"""Traceback (most recent call last):
  File "{case['error_file']}", line 1, in <module>
{case['error_message']}
"""
        return buggy_code, error_traceback, None
    
    else:
        # çœŸå®bugï¼šä»BugsinPyåŠ è½½
        base_path = "data/BugsInPy-master"
        project_path = os.path.join(base_path, case['project_path'])
        error_file = case['error_file']
        buggy_file_path = os.path.join(project_path, error_file)
        
        with open(buggy_file_path, 'r') as f:
            buggy_code = f.read()
        
        error_traceback = f"""Traceback (most recent call last):
  File "{error_file}", line 10, in <module>
    some_function()
NameError: name '{case['undefined_name']}' is not defined
"""
        return buggy_code, error_traceback, project_path


# ============== Checkpointç®¡ç†ï¼ˆä¸å˜ï¼‰==============

def load_checkpoint() -> Dict:
    """åŠ è½½checkpoint"""
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            return json.load(f)
    return {"completed": [], "results": {}}


def save_checkpoint(checkpoint: Dict):
    """ä¿å­˜checkpoint"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(checkpoint, f, indent=2)


# ============== æ”¹è¿›ï¼šä¿å­˜è¯¦ç»†ç»“æœ ==============

def save_detailed_result(result: Dict):
    """
    ä¿å­˜è¯¦ç»†ç»“æœåˆ°å•ç‹¬æ–‡ä»¶
    
    ç»“æ„ï¼š
    - detailed_results/{case_id}_{agent}_{run}.json  # æ‰€æœ‰ç»“æœ
    - failures/{case_id}_{agent}_{run}.json         # åªæœ‰å¤±è´¥
    """
    os.makedirs(DETAILED_DIR, exist_ok=True)
    os.makedirs(FAILURES_DIR, exist_ok=True)
    
    # 1. ä¿å­˜åˆ°detailed_results
    detailed_file = os.path.join(DETAILED_DIR, f"{result['test_id']}.json")
    with open(detailed_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    # 2. å¦‚æœå¤±è´¥ï¼Œé¢å¤–ä¿å­˜åˆ°failuresç›®å½•
    if not result['success']:
        failure_file = os.path.join(FAILURES_DIR, f"{result['test_id']}.json")
        
        # æ·»åŠ é¢å¤–çš„è°ƒè¯•ä¿¡æ¯
        failure_detail = {
            **result,
            'analysis': analyze_failure(result)  # è‡ªåŠ¨åˆ†æå¤±è´¥åŸå› 
        }
        
        with open(failure_file, 'w', encoding='utf-8') as f:
            json.dump(failure_detail, f, indent=2, ensure_ascii=False)


def analyze_failure(result: Dict) -> Dict:
    """
    è‡ªåŠ¨åˆ†æå¤±è´¥åŸå› 
    
    è¿”å›ï¼š
    {
        'likely_reason': '...',
        'llm_last_thought': '...',
        'suggestions': [...]
    }
    """
    analysis = {
        'likely_reason': 'Unknown',
        'llm_last_action': None,
        'suggestions': []
    }
    
    agent_type = result['agent']
    
    if agent_type == 'router':
        # Routerå¤±è´¥åˆ†æ
        if 'error' in result:
            analysis['likely_reason'] = f"Exception: {result['error']}"
        elif 'all_attempts' in result:
            attempts = result['all_attempts']
            if attempts:
                last_attempt = attempts[-1]
                if 'execution_result' in last_attempt:
                    exec_result = last_attempt['execution_result']
                    if not exec_result.get('success'):
                        analysis['likely_reason'] = "Dockeræ‰§è¡Œå¤±è´¥"
                        analysis['llm_last_action'] = last_attempt.get('explanation', '')
                else:
                    analysis['likely_reason'] = "LLMç”Ÿæˆä¿®å¤å¤±è´¥"
                    analysis['llm_last_action'] = last_attempt.get('explanation', '')
        
        analysis['suggestions'] = [
            "æ£€æŸ¥LLMç”Ÿæˆçš„ä»£ç æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯",
            "æ£€æŸ¥æ˜¯å¦è¶…å‡ºæœ€å¤§é‡è¯•æ¬¡æ•°",
            "æŸ¥çœ‹all_attemptsä¸­çš„æ¯æ¬¡å°è¯•"
        ]
    
    elif agent_type == 'react':
        # ReActå¤±è´¥åˆ†æ
        if 'error' in result:
            analysis['likely_reason'] = f"Exception: {result['error']}"
        elif 'react_history' in result:
            history = result['react_history']
            if history:
                last_step = history[-1]
                analysis['llm_last_thought'] = last_step.get('thought', '')
                analysis['llm_last_action'] = last_step.get('action', '')
                
                # åˆ¤æ–­å¤±è´¥åŸå› 
                if result.get('iterations', 0) >= 15:
                    analysis['likely_reason'] = "è¶…å‡ºæœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆ15æ¬¡ï¼‰"
                elif 'execute_code' not in str(history):
                    analysis['likely_reason'] = "LLMæœªè°ƒç”¨execute_codeéªŒè¯"
                else:
                    analysis['likely_reason'] = "LLMå†³ç­–é”™è¯¯æˆ–ç”Ÿæˆä»£ç æœ‰é—®é¢˜"
        
        analysis['suggestions'] = [
            "æ£€æŸ¥react_historyä¸­çš„æ€è€ƒè¿‡ç¨‹",
            "æŸ¥çœ‹æ˜¯å¦æœ‰é‡å¤çš„Toolè°ƒç”¨",
            "ç¡®è®¤æ˜¯å¦è°ƒç”¨äº†execute_codeéªŒè¯"
        ]
    
    return analysis


# ============== å•æ¬¡æµ‹è¯•å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰==============

def test_with_router_single(case: Dict, run_num: int) -> Dict[str, Any]:
    """ç”¨Router Agentæµ‹è¯•å•ä¸ªæ¡ˆä¾‹ï¼ˆå•æ¬¡è¿è¡Œï¼‰"""
    
    case_id = case['id']
    test_id = f"{case_id}_router_run{run_num}"
    
    print(f"  ğŸ”· [{case['name']}] Router Run {run_num}...", end=" ", flush=True)
    
    start_time = time.time()
    
    try:
        # å‡†å¤‡è¾“å…¥
        buggy_code, error_traceback, project_path = prepare_test_input(case)
        
        # åˆ›å»ºRouter Agent
        agent = RouterAgent(
            api_key=api_key,
            project_path=project_path
        )
        
        # ç¦ç”¨æ—¥å¿—è¾“å‡º
        import logging
        logging.getLogger('src.agent').setLevel(logging.WARNING)
        logging.getLogger('src.agent.tools').setLevel(logging.WARNING)
        
        # æ‰§è¡Œdebug
        result = agent.debug(
            buggy_code=buggy_code,
            error_traceback=error_traceback,
            max_retries=2
        )
        
        elapsed = time.time() - start_time
        success = result.get('success', False)
        
        print(f"{'âœ…' if success else 'âŒ'} {elapsed:.1f}s")
        
        # è¯¦ç»†è®°å½•
        detailed_result = {
            'test_id': test_id,
            'case_id': case_id,
            'case_name': case['name'],
            'agent': 'router',
            'run_num': run_num,
            'source': case['source'],
            'error_type': case.get('error_type', 'Unknown'),
            'success': success,
            'time': elapsed,
            'attempts': len(result.get('all_attempts', [])),
            'fixed_code': result.get('fixed_code', ''),
            'all_attempts': result.get('all_attempts', []),  # å®Œæ•´çš„é‡è¯•å†å²
            'original_error': error_traceback,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        save_detailed_result(detailed_result)
        
        return detailed_result
        
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"âŒ ERROR {elapsed:.1f}s")
        
        error_result = {
            'test_id': test_id,
            'case_id': case_id,
            'case_name': case['name'],
            'agent': 'router',
            'run_num': run_num,
            'source': case['source'],
            'error_type': case.get('error_type', 'Unknown'),
            'success': False,
            'time': elapsed,
            'error': str(e),
            'error_detail': traceback.format_exc(),
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        save_detailed_result(error_result)
        
        return error_result


def test_with_react_single(case: Dict, run_num: int) -> Dict[str, Any]:
    """ç”¨ReAct Agentæµ‹è¯•å•ä¸ªæ¡ˆä¾‹ï¼ˆå•æ¬¡è¿è¡Œï¼‰"""
    
    case_id = case['id']
    test_id = f"{case_id}_react_run{run_num}"
    
    print(f"  ğŸ”¶ [{case['name']}] ReAct Run {run_num}...", end=" ", flush=True)
    
    start_time = time.time()
    
    try:
        # å‡†å¤‡è¾“å…¥
        buggy_code, error_traceback, project_path = prepare_test_input(case)
        
        # åˆ›å»ºReAct Agent
        agent = ReActAgent(
            api_key=api_key,
            max_iterations=15,
            temperature=0.1
        )
        
        # ç¦ç”¨æ—¥å¿—è¾“å‡º
        import logging
        logging.getLogger('src.agent').setLevel(logging.WARNING)
        logging.getLogger('src.agent.tools').setLevel(logging.WARNING)
        
        # æ‰§è¡Œdebug
        result = agent.debug(
            buggy_code=buggy_code,
            error_traceback=error_traceback,
            project_path=project_path,
            max_iterations=15
        )
        
        elapsed = time.time() - start_time
        success = result.get('success', False)
        iterations = result.get('iterations', 0)
        
        print(f"{'âœ…' if success else 'âŒ'} {elapsed:.1f}s ({iterations} iter)")
        
        # è¯¦ç»†è®°å½•
        detailed_result = {
            'test_id': test_id,
            'case_id': case_id,
            'case_name': case['name'],
            'agent': 'react',
            'run_num': run_num,
            'source': case['source'],
            'error_type': case.get('error_type', 'Unknown'),
            'success': success,
            'time': elapsed,
            'iterations': iterations,
            'fixed_code': result.get('fixed_code', ''),
            'react_history': result.get('history', []),  # å®Œæ•´çš„æ€è€ƒå†å²
            'original_error': error_traceback,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        save_detailed_result(detailed_result)
        
        return detailed_result
        
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"âŒ ERROR {elapsed:.1f}s")
        
        error_result = {
            'test_id': test_id,
            'case_id': case_id,
            'case_name': case['name'],
            'agent': 'react',
            'run_num': run_num,
            'source': case['source'],
            'error_type': case.get('error_type', 'Unknown'),
            'success': False,
            'time': elapsed,
            'error': str(e),
            'error_detail': traceback.format_exc(),
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        save_detailed_result(error_result)
        
        return error_result


# ============== ä¸»æµ‹è¯•æµç¨‹ï¼ˆæ”¹è¿›ï¼‰==============

def run_batch_test(fresh_start: bool = False):
    """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
    print("=" * 80)
    print("ğŸ¥Š Router vs ReAct - Batch Comparison Test V2")
    print("=" * 80)
    
    # æ¸…é™¤æ—§æ•°æ®
    if fresh_start:
        print("ğŸ—‘ï¸  Fresh start - removing old data...")
        if os.path.exists(CHECKPOINT_FILE):
            os.remove(CHECKPOINT_FILE)
        if os.path.exists(DETAILED_DIR):
            import shutil
            shutil.rmtree(DETAILED_DIR)
        if os.path.exists(FAILURES_DIR):
            import shutil
            shutil.rmtree(FAILURES_DIR)
        print("   âœ… Old data cleared\n")
    
    # åŠ è½½æµ‹è¯•æ¡ˆä¾‹
    test_cases = load_all_test_cases()
    
    # åŠ è½½checkpoint
    checkpoint = load_checkpoint()
    
    print(f"âš™ï¸  Config: {NUM_RUNS} runs Ã— 2 agents = {len(test_cases) * NUM_RUNS * 2} tests")
    print(f"ğŸš€ Parallel: {MAX_WORKERS} workers")
    print(f"â±ï¸  Estimated: ~{(len(test_cases) * NUM_RUNS * 2 / MAX_WORKERS * 30 / 60):.0f}min\n")
    
    # é¢„åˆå§‹åŒ–Embedder
    print("ğŸ”§ Pre-initializing Embedder...")
    from src.rag.embedder import get_embedder_instance
    try:
        embedder = get_embedder_instance()
        print(f"âœ… Embedder ready (dim={embedder.embedding_dim})\n")
    except Exception as e:
        print(f"âŒ Embedder failed: {e}")
        return
    
    # ç”Ÿæˆæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    all_tasks = []
    for case in test_cases:
        for agent_type in ['router', 'react']:
            for run_num in range(1, NUM_RUNS + 1):
                test_id = f"{case['id']}_{agent_type}_run{run_num}"
                
                # è·³è¿‡å·²å®Œæˆ
                if test_id in checkpoint['completed']:
                    continue
                
                all_tasks.append((case, agent_type, run_num))
    
    total_tests = len(test_cases) * NUM_RUNS * 2
    completed_count = len(checkpoint['completed'])
    
    print(f"ğŸ“Š Progress: {completed_count}/{total_tests} completed")
    print(f"ğŸ”„ Remaining: {len(all_tasks)} tasks\n")
    
    if len(all_tasks) == 0:
        print("âœ… All tests already completed!")
        generate_summary(checkpoint, test_cases)
        return
    
    # å¹¶è¡Œæ‰§è¡Œ
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {}
        for case, agent_type, run_num in all_tasks:
            if agent_type == 'router':
                future = executor.submit(test_with_router_single, case, run_num)
            else:
                future = executor.submit(test_with_react_single, case, run_num)
            
            future_to_task[future] = (case['id'], agent_type, run_num)
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_task):
            case_id, agent_type, run_num = future_to_task[future]
            
            try:
                result = future.result()
                
                # ä¿å­˜ç»“æœï¼ˆåªä¿å­˜ç®€è¦ä¿¡æ¯åˆ°checkpointï¼‰
                test_id = result['test_id']
                checkpoint['results'][test_id] = {
                    'test_id': test_id,
                    'success': result['success'],
                    'time': result['time'],
                    'agent': result['agent']
                }
                checkpoint['completed'].append(test_id)
                
                # ä¿å­˜checkpoint
                save_checkpoint(checkpoint)
                
                # æ‰“å°è¿›åº¦
                completed_count = len(checkpoint['completed'])
                progress = (completed_count / total_tests) * 100
                
                print(f"[{completed_count}/{total_tests}] {progress:.1f}%")
                
            except Exception as e:
                print(f"âŒ Task failed: {case_id}_{agent_type}_run{run_num}: {e}")
    
    elapsed = time.time() - start_time
    
    print(f"\nâœ… All tests completed in {elapsed/60:.1f}min")
    print("=" * 80)
    
    # ç”Ÿæˆæ‘˜è¦
    generate_summary(checkpoint, test_cases)


# ============== æ”¹è¿›ï¼šç”Ÿæˆç´§å‡‘æ‘˜è¦ ==============

def generate_summary(checkpoint: Dict, test_cases: List[Dict]):
    """
    ç”Ÿæˆç´§å‡‘çš„æ‘˜è¦æŠ¥å‘Š
    
    è¾“å‡ºï¼š
    1. summary.json - åªæœ‰ç»Ÿè®¡æ•°æ®ï¼ˆå°æ–‡ä»¶ï¼‰
    2. failures_summary.md - å¤±è´¥æ¡ˆä¾‹æ¸…å•ï¼ˆMarkdownï¼‰
    """
    print("\nğŸ“Š Generating summary...")
    
    # æ”¶é›†æ‰€æœ‰è¯¦ç»†ç»“æœ
    all_results = []
    for filename in os.listdir(DETAILED_DIR):
        if filename.endswith('.json'):
            with open(os.path.join(DETAILED_DIR, filename), 'r') as f:
                all_results.append(json.load(f))
    
    # æŒ‰agentåˆ†ç»„
    router_results = [r for r in all_results if r['agent'] == 'router']
    react_results = [r for r in all_results if r['agent'] == 'react']
    
    # åŸºç¡€ç»Ÿè®¡
    router_stats = calculate_stats(router_results)
    react_stats = calculate_stats(react_results)
    
    # æŒ‰æ¥æºç»Ÿè®¡
    router_by_source = {
        'constructed': [r for r in router_results if r['source'] == 'constructed'],
        'bugsinpy': [r for r in router_results if r['source'] == 'bugsinpy']
    }
    react_by_source = {
        'constructed': [r for r in react_results if r['source'] == 'constructed'],
        'bugsinpy': [r for r in react_results if r['source'] == 'bugsinpy']
    }
    
    # å¤±è´¥æ¡ˆä¾‹
    router_failures = [r for r in router_results if not r['success']]
    react_failures = [r for r in react_results if not r['success']]
    
    # ç»„è£…ç´§å‡‘æ‘˜è¦
    summary = {
        'timestamp': datetime.now().isoformat(),
        'config': {
            'num_runs': NUM_RUNS,
            'total_cases': len(test_cases),
            'total_tests': len(all_results)
        },
        'overall': {
            'router': router_stats,
            'react': react_stats
        },
        'by_source': {
            'router': {
                'constructed': calculate_stats(router_by_source['constructed']),
                'bugsinpy': calculate_stats(router_by_source['bugsinpy'])
            },
            'react': {
                'constructed': calculate_stats(react_by_source['constructed']),
                'bugsinpy': calculate_stats(react_by_source['bugsinpy'])
            }
        },
        'failures': {
            'router': [r['test_id'] for r in router_failures],
            'react': [r['test_id'] for r in react_failures]
        },
        'failure_by_error_type': analyze_failures_by_error_type(all_results)
    }
    
    # ä¿å­˜ç´§å‡‘æ‘˜è¦
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Summary saved: {SUMMARY_FILE}")
    
    # ç”ŸæˆMarkdownå¤±è´¥æŠ¥å‘Š
    generate_failures_markdown(router_failures, react_failures)
    
    # æ‰“å°æ‘˜è¦
    print_summary(summary)


def analyze_failures_by_error_type(results: List[Dict]) -> Dict:
    """æŒ‰é”™è¯¯ç±»å‹åˆ†æå¤±è´¥æ¡ˆä¾‹"""
    failure_by_type = {}
    
    for r in results:
        if not r['success']:
            error_type = r.get('error_type', 'Unknown')
            if error_type not in failure_by_type:
                failure_by_type[error_type] = []
            failure_by_type[error_type].append(r['test_id'])
    
    return failure_by_type


def generate_failures_markdown(router_failures: List[Dict], react_failures: List[Dict]):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„å¤±è´¥æ¡ˆä¾‹æŠ¥å‘Š
    
    è¾“å‡ºï¼šfailures_summary.md
    """
    md_content = f"""# å¤±è´¥æ¡ˆä¾‹åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š æ€»ä½“æƒ…å†µ

- **Routerå¤±è´¥**: {len(router_failures)} ä¸ª
- **ReActå¤±è´¥**: {len(react_failures)} ä¸ª
- **ReActé¢å¤–å¤±è´¥**: {len(react_failures) - len(router_failures)} ä¸ª

---

## ğŸ”· Routerå¤±è´¥æ¡ˆä¾‹ ({len(router_failures)}ä¸ª)

"""
    
    for failure in sorted(router_failures, key=lambda x: x['test_id']):
        md_content += f"""### {failure['test_id']}

- **æ¡ˆä¾‹å**: {failure['case_name']}
- **é”™è¯¯ç±»å‹**: {failure.get('error_type', 'Unknown')}
- **è€—æ—¶**: {failure['time']:.1f}s
- **å°è¯•æ¬¡æ•°**: {failure.get('attempts', 0)}
- **å¤±è´¥åŸå› **: {failure.get('analysis', {}).get('likely_reason', 'æœªåˆ†æ')}

**LLMæœ€åä¸€æ¬¡å°è¯•**:
```
{failure.get('analysis', {}).get('llm_last_action', 'æ— ')}
```

**è¯¦ç»†ç»“æœ**: `detailed_results/{failure['test_id']}.json`

---

"""
    
    md_content += f"""
## ğŸ”¶ ReActå¤±è´¥æ¡ˆä¾‹ ({len(react_failures)}ä¸ª)

"""
    
    for failure in sorted(react_failures, key=lambda x: x['test_id']):
        md_content += f"""### {failure['test_id']}

- **æ¡ˆä¾‹å**: {failure['case_name']}
- **é”™è¯¯ç±»å‹**: {failure.get('error_type', 'Unknown')}
- **è€—æ—¶**: {failure['time']:.1f}s
- **è¿­ä»£æ¬¡æ•°**: {failure.get('iterations', 0)}
- **å¤±è´¥åŸå› **: {failure.get('analysis', {}).get('likely_reason', 'æœªåˆ†æ')}

**LLMæœ€åçš„æ€è€ƒ**:
```
{failure.get('analysis', {}).get('llm_last_thought', 'æ— ')}
```

**LLMæœ€åçš„è¡ŒåŠ¨**:
```
{failure.get('analysis', {}).get('llm_last_action', 'æ— ')}
```

**è¯¦ç»†ç»“æœ**: `detailed_results/{failure['test_id']}.json`

---

"""
    
    # ä¿å­˜
    with open(FAILURES_MD_FILE, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"âœ… Failures summary saved: {FAILURES_MD_FILE}")


def calculate_stats(results: List[Dict]) -> Dict:
    """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
    if not results:
        return {}
    
    successes = [r for r in results if r.get('success')]
    times = [r.get('time', 0) for r in results]
    
    return {
        'total_runs': len(results),
        'success_count': len(successes),
        'success_rate': len(successes) / len(results) * 100,
        'avg_time': sum(times) / len(times),
        'min_time': min(times),
        'max_time': max(times)
    }


def print_summary(summary: Dict):
    """æ‰“å°æµ‹è¯•æ‘˜è¦"""
    print("\n" + "=" * 80)
    print("ğŸ“Š COMPARISON SUMMARY")
    print("=" * 80)
    
    router = summary['overall']['router']
    react = summary['overall']['react']
    
    print(f"\nâœ… SUCCESS RATE:")
    print(f"  Router: {router['success_rate']:.1f}% ({router['success_count']}/{router['total_runs']})")
    print(f"  ReAct:  {react['success_rate']:.1f}% ({react['success_count']}/{react['total_runs']})")
    
    print(f"\nâ±ï¸  TIME:")
    print(f"  Router: avg={router['avg_time']:.1f}s, min={router['min_time']:.1f}s, max={router['max_time']:.1f}s")
    print(f"  ReAct:  avg={react['avg_time']:.1f}s, min={react['min_time']:.1f}s, max={react['max_time']:.1f}s")
    
    # å¤±è´¥æ¡ˆä¾‹
    print(f"\nâŒ FAILURES:")
    print(f"  Router: {len(summary['failures']['router'])} cases")
    for test_id in summary['failures']['router']:
        print(f"    - {test_id}")
    
    print(f"  ReAct:  {len(summary['failures']['react'])} cases")
    for test_id in summary['failures']['react']:
        print(f"    - {test_id}")
    
    print(f"\nğŸ“ è¯¦ç»†å¤±è´¥åˆ†æè¯·æŸ¥çœ‹: {FAILURES_MD_FILE}")
    print("=" * 80)


# ============== ä¸»å…¥å£ ==============

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Router vs ReAct æ‰¹é‡å¯¹æ¯”æµ‹è¯•')
    parser.add_argument('--fresh-start', action='store_true', help='æ¸…é™¤æ—§æ•°æ®ï¼Œé‡æ–°å¼€å§‹')
    args = parser.parse_args()
    
    try:
        run_batch_test(fresh_start=args.fresh_start)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted - progress saved to checkpoint")
        print("    Re-run to continue from where you left off")
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        traceback.print_exc()