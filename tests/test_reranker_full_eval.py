"""
Rerankerå®Œæ•´è¯„ä¼°

ç›®æ ‡ï¼š
- ç”¨30ä¸ªçœŸå®queryæµ‹è¯•BaseRetriever vs Reranker
- è®¡ç®—Recall@5å¯¹æ¯”
- åˆ†ææ€§èƒ½å·®å¼‚

é¢„è®¡è€—æ—¶ï¼š20-30åˆ†é’Ÿ
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import time
import logging
import json
from typing import List, Dict, Any
from collections import defaultdict
import chromadb
from sentence_transformers import CrossEncoder

from src.rag.retriever import BaseRetriever
from src.rag.embedder import Embedder

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é…ç½®
CONFIG = {
    'test_file': 'data/test_cases/test_queries_realistic.json',
    'vectorstore_path': 'data/vectorstore/chroma_s1',
    'embedding_model': 'BAAI/bge-small-en-v1.5',
    'reranker_model': 'tomaarsen/Qwen3-Reranker-0.6B-seq-cls',
    'recall_k': 40,  # å¬å›40ä¸ª
    'top_k': 5       # è¯„ä¼°Top 5
}


def load_test_queries(test_file: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½æµ‹è¯•æŸ¥è¯¢
    
    Args:
        test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„
        
    Returns:
        æŸ¥è¯¢åˆ—è¡¨
        
    Raises:
        FileNotFoundError: å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
        ValueError: å¦‚æœæ ¼å¼ä¸æ­£ç¡®
    """
    test_path = Path(test_file)
    
    if not test_path.exists():
        raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_path}")
    
    logger.info(f"ğŸ“‚ åŠ è½½æµ‹è¯•é›†: {test_file}")
    
    with open(test_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if 'queries' not in data:
        raise ValueError("æµ‹è¯•æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘'queries'å­—æ®µ")
    
    queries = data['queries']
    logger.info(f"âœ… åŠ è½½æˆåŠŸï¼Œå…± {len(queries)} ä¸ªæµ‹è¯•query")
    
    return queries


def load_models() -> tuple[BaseRetriever, CrossEncoder]:
    """
    åŠ è½½æ£€ç´¢å™¨å’ŒReranker
    
    Returns:
        (base_retriever, reranker)
    """
    logger.info("ğŸ¤– åŠ è½½æ¨¡å‹...")
    
    # 1. åŠ è½½å‘é‡æ•°æ®åº“
    vectorstore_path = Path(CONFIG['vectorstore_path'])
    client = chromadb.PersistentClient(path=str(vectorstore_path))
    collection = client.get_collection(name="stackoverflow_kb")
    
    logger.info(f"âœ… VectorstoreåŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°ï¼š{collection.count()}")
    
    # 2. åŠ è½½Embedder
    embedder = Embedder(model_name=CONFIG['embedding_model'])
    logger.info(f"âœ… EmbedderåŠ è½½æˆåŠŸ")
    
    # 3. åˆ›å»ºBaseRetriever
    retriever = BaseRetriever(
        collection=collection,
        embedding_function=embedder,
        min_similarity=0.5,
        recall_factor=4
    )
    logger.info("âœ… BaseRetrieveråˆ›å»ºæˆåŠŸ")
    
    # 4. åŠ è½½Qwen3-Reranker
    reranker = CrossEncoder(CONFIG['reranker_model'])
    logger.info(f"âœ… Qwen3-RerankeråŠ è½½æˆåŠŸ: {CONFIG['reranker_model']}")
        
    return retriever, reranker


def calculate_recall_at_k(
    retrieved_ids: List[str],
    ground_truth_ids: List[str],
    k: int
) -> float:
    """
    è®¡ç®—Recall@K
    
    Args:
        retrieved_ids: æ£€ç´¢åˆ°çš„æ–‡æ¡£IDåˆ—è¡¨
        ground_truth_ids: çœŸå®ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨
        k: è¯„ä¼°å‰Kä¸ª
        
    Returns:
        Recall@Kåˆ†æ•°
    """
    if not ground_truth_ids:
        return 0.0
    
    # åªçœ‹å‰Kä¸ª
    top_k_ids = retrieved_ids[:k]
    
    # æå–æ–‡æ¡£IDï¼ˆå»æ‰chunkåç¼€ï¼‰
    def extract_doc_id(doc_id: str) -> str:
        """æå–æ–‡æ¡£IDï¼ˆå»æ‰_chunk_Xï¼‰"""
        if '_chunk_' in doc_id:
            return doc_id.split('_chunk_')[0]
        return doc_id
    
    top_k_doc_ids = {extract_doc_id(id) for id in top_k_ids}
    gt_doc_ids = {extract_doc_id(id) for id in ground_truth_ids}
    
    # è®¡ç®—å¬å›
    hits = len(top_k_doc_ids & gt_doc_ids)
    recall = hits / len(gt_doc_ids)
    
    return recall


def evaluate_retriever(
    retriever: BaseRetriever,
    test_queries: List[Dict[str, Any]],
    top_k: int,
    name: str = "BaseRetriever"
) -> Dict[str, Any]:
    """
    è¯„ä¼°æ£€ç´¢å™¨
    
    Args:
        retriever: æ£€ç´¢å™¨
        test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
        top_k: è¯„ä¼°å‰Kä¸ª
        name: æ£€ç´¢å™¨åç§°
        
    Returns:
        è¯„ä¼°ç»“æœ
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ” è¯„ä¼° {name}")
    logger.info(f"{'='*80}")
    
    recalls = []
    total_time = 0
    detailed_results = []
    
    for i, test_case in enumerate(test_queries, 1):
        query = test_case['query']
        ground_truth = test_case['ground_truth']
        
        # æ£€ç´¢
        start_time = time.time()
        results = retriever.search(query, top_k=CONFIG['recall_k'])
        query_time = time.time() - start_time
        total_time += query_time
        
        # æå–ID
        retrieved_ids = [r['id'] for r in results]
        
        # è®¡ç®—Recall@K
        recall = calculate_recall_at_k(retrieved_ids, ground_truth, top_k)
        recalls.append(recall)
        
        # è®°å½•è¯¦ç»†ç»“æœ
        detailed_results.append({
            'query_id': test_case['query_id'],
            'query': query,
            'ground_truth': ground_truth,
            'retrieved_top5': retrieved_ids[:top_k],
            'recall': recall,
            'time_ms': query_time * 1000
        })
        
        # æ¯10ä¸ªæ‰“å°ä¸€æ¬¡è¿›åº¦
        if i % 10 == 0:
            logger.info(f"  è¿›åº¦: {i}/{len(test_queries)}, å½“å‰å¹³å‡Recall@{top_k}: {sum(recalls)/len(recalls):.4f}")
    
    # ç»Ÿè®¡
    avg_recall = sum(recalls) / len(recalls) if recalls else 0.0
    avg_time = total_time / len(test_queries) if test_queries else 0.0
    
    logger.info(f"\nğŸ“Š {name} ç»“æœ:")
    logger.info(f"   Recall@{top_k}: {avg_recall:.4f}")
    logger.info(f"   å¹³å‡è€—æ—¶: {avg_time*1000:.2f}ms")
    logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    return {
        'name': name,
        'recall_at_k': avg_recall,
        'avg_time_ms': avg_time * 1000,
        'total_time_s': total_time,
        'detailed_results': detailed_results
    }


def evaluate_reranker(
    retriever: BaseRetriever,
    reranker: CrossEncoder,
    test_queries: List[Dict[str, Any]],
    top_k: int
) -> Dict[str, Any]:
    """
    è¯„ä¼°Reranker
    
    Args:
        retriever: åŸºç¡€æ£€ç´¢å™¨
        reranker: Rerankeræ¨¡å‹
        test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
        top_k: è¯„ä¼°å‰Kä¸ª
        
    Returns:
        è¯„ä¼°ç»“æœ
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ¯ è¯„ä¼° Reranker")
    logger.info(f"{'='*80}")
    
    recalls = []
    total_retrieval_time = 0
    total_rerank_time = 0
    detailed_results = []
    
    for i, test_case in enumerate(test_queries, 1):
        query = test_case['query']
        ground_truth = test_case['ground_truth']
        
        # 1. åŸºç¡€æ£€ç´¢
        start_time = time.time()
        candidates = retriever.search(query, top_k=CONFIG['recall_k'])
        retrieval_time = time.time() - start_time
        total_retrieval_time += retrieval_time
        
        if not candidates:
            recalls.append(0.0)
            continue
        
        # 2. Rerank
        pairs = [[query, doc['content']] for doc in candidates]
        
        start_time = time.time()
        try:
            scores = reranker.predict(pairs)  
            if hasattr(scores, 'tolist'):
                scores = scores.tolist()
            elif not isinstance(scores, list):
                scores = [float(scores)]
        except Exception as e:
            logger.error(f"Query {i} Rerankå¤±è´¥: {e}")
            recalls.append(0.0)
            continue
        
        rerank_time = time.time() - start_time
        total_rerank_time += rerank_time
        
        # 3. å¤„ç†åˆ†æ•°
        if not isinstance(scores, list):
            scores = [scores]
        
        # 4. æ·»åŠ åˆ†æ•°å¹¶æ’åº
        for doc, score in zip(candidates, scores):
            doc['rerank_score'] = float(score)
        
        reranked = sorted(
            candidates,
            key=lambda x: x['rerank_score'],
            reverse=True
        )
        
        # 5. æå–ID
        retrieved_ids = [r['id'] for r in reranked]
        
        # 6. è®¡ç®—Recall@K
        recall = calculate_recall_at_k(retrieved_ids, ground_truth, top_k)
        recalls.append(recall)
        
        # 7. è®°å½•è¯¦ç»†ç»“æœ
        detailed_results.append({
            'query_id': test_case['query_id'],
            'query': query,
            'ground_truth': ground_truth,
            'retrieved_top5': retrieved_ids[:top_k],
            'recall': recall,
            'retrieval_time_ms': retrieval_time * 1000,
            'rerank_time_ms': rerank_time * 1000,
            'total_time_ms': (retrieval_time + rerank_time) * 1000
        })
        
        # æ¯10ä¸ªæ‰“å°ä¸€æ¬¡è¿›åº¦
        if i % 10 == 0:
            logger.info(f"  è¿›åº¦: {i}/{len(test_queries)}, å½“å‰å¹³å‡Recall@{top_k}: {sum(recalls)/len(recalls):.4f}")
    
    # ç»Ÿè®¡
    avg_recall = sum(recalls) / len(recalls) if recalls else 0.0
    avg_retrieval_time = total_retrieval_time / len(test_queries) if test_queries else 0.0
    avg_rerank_time = total_rerank_time / len(test_queries) if test_queries else 0.0
    avg_total_time = avg_retrieval_time + avg_rerank_time
    
    logger.info(f"\nğŸ“Š Reranker ç»“æœ:")
    logger.info(f"   Recall@{top_k}: {avg_recall:.4f}")
    logger.info(f"   å¹³å‡æ£€ç´¢è€—æ—¶: {avg_retrieval_time*1000:.2f}ms")
    logger.info(f"   å¹³å‡Rerankè€—æ—¶: {avg_rerank_time*1000:.2f}ms")
    logger.info(f"   å¹³å‡æ€»è€—æ—¶: {avg_total_time*1000:.2f}ms")
    logger.info(f"   æ€»è€—æ—¶: {(total_retrieval_time + total_rerank_time):.2f}ç§’")
    
    return {
        'name': 'Reranker',
        'recall_at_k': avg_recall,
        'avg_retrieval_time_ms': avg_retrieval_time * 1000,
        'avg_rerank_time_ms': avg_rerank_time * 1000,
        'avg_total_time_ms': avg_total_time * 1000,
        'total_time_s': total_retrieval_time + total_rerank_time,
        'detailed_results': detailed_results
    }


def compare_results(
    base_result: Dict[str, Any],
    rerank_result: Dict[str, Any],
    top_k: int
) -> None:
    """
    å¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„ç»“æœ
    
    Args:
        base_result: BaseRetrieverç»“æœ
        rerank_result: Rerankerç»“æœ
        top_k: è¯„ä¼°çš„Kå€¼
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“Š å¯¹æ¯”åˆ†æ")
    logger.info(f"{'='*80}")
    
    # 1. æ•´ä½“å¯¹æ¯”
    base_recall = base_result['recall_at_k']
    rerank_recall = rerank_result['recall_at_k']
    diff = rerank_recall - base_recall
    diff_pct = (diff / base_recall * 100) if base_recall > 0 else 0
    
    logger.info(f"\nğŸ¯ Recall@{top_k} å¯¹æ¯”:")
    logger.info(f"   BaseRetriever:  {base_recall:.4f}")
    logger.info(f"   Reranker:       {rerank_recall:.4f}")
    logger.info(f"   å·®å¼‚:           {diff:+.4f} ({diff_pct:+.2f}%)")
    
    if diff > 0:
        logger.info(f"   âœ… Rerankeræå‡äº† {diff_pct:.2f}%")
    elif diff < 0:
        logger.info(f"   âŒ Rerankeré™ä½äº† {abs(diff_pct):.2f}%")
    else:
        logger.info(f"   âš–ï¸  ä¸¤è€…ç›¸åŒ")
    
    # 2. æ€§èƒ½å¯¹æ¯”
    base_time = base_result['avg_time_ms']
    rerank_time = rerank_result['avg_total_time_ms']
    time_ratio = rerank_time / base_time if base_time > 0 else 0
    
    logger.info(f"\nâ±ï¸  æ€§èƒ½å¯¹æ¯”:")
    logger.info(f"   BaseRetriever:  {base_time:.2f}ms")
    logger.info(f"   Reranker:       {rerank_time:.2f}ms")
    logger.info(f"   æ…¢äº†:           {time_ratio:.1f}å€")
    
    # 3. é€queryåˆ†æ
    logger.info(f"\nğŸ“ˆ é€Queryåˆ†æ:")
    
    better = 0
    worse = 0
    same = 0
    
    for base_detail, rerank_detail in zip(
        base_result['detailed_results'],
        rerank_result['detailed_results']
    ):
        base_r = base_detail['recall']
        rerank_r = rerank_detail['recall']
        
        if rerank_r > base_r:
            better += 1
        elif rerank_r < base_r:
            worse += 1
        else:
            same += 1
    
    total = len(base_result['detailed_results'])
    logger.info(f"   æ›´å¥½: {better}/{total} ({better/total*100:.1f}%)")
    logger.info(f"   æ›´å·®: {worse}/{total} ({worse/total*100:.1f}%)")
    logger.info(f"   ç›¸åŒ: {same}/{total} ({same/total*100:.1f}%)")
    
    # 4. æ‰¾å‡ºå˜åŒ–æœ€å¤§çš„cases
    logger.info(f"\nğŸ” å˜åŒ–æœ€å¤§çš„cases:")
    
    changes = []
    for base_detail, rerank_detail in zip(
        base_result['detailed_results'],
        rerank_result['detailed_results']
    ):
        change = rerank_detail['recall'] - base_detail['recall']
        changes.append({
            'query_id': base_detail['query_id'],
            'query': base_detail['query'][:50],
            'base_recall': base_detail['recall'],
            'rerank_recall': rerank_detail['recall'],
            'change': change
        })
    
    # æ’åºï¼šå˜åŒ–æœ€å¤§çš„ï¼ˆæ­£å‘å’Œè´Ÿå‘å„5ä¸ªï¼‰
    changes_sorted = sorted(changes, key=lambda x: x['change'], reverse=True)
    
    logger.info(f"\n   ğŸ“ˆ æå‡æœ€å¤§çš„5ä¸ª:")
    for i, c in enumerate(changes_sorted[:5], 1):
        if c['change'] > 0:
            logger.info(
                f"      {i}. {c['query_id']}: {c['query']}... "
                f"({c['base_recall']:.2f} â†’ {c['rerank_recall']:.2f}, +{c['change']:.2f})"
            )
    
    logger.info(f"\n   ğŸ“‰ ä¸‹é™æœ€å¤§çš„5ä¸ª:")
    for i, c in enumerate(changes_sorted[-5:][::-1], 1):
        if c['change'] < 0:
            logger.info(
                f"      {i}. {c['query_id']}: {c['query']}... "
                f"({c['base_recall']:.2f} â†’ {c['rerank_recall']:.2f}, {c['change']:.2f})"
            )


def save_results(
    base_result: Dict[str, Any],
    rerank_result: Dict[str, Any]
) -> None:
    """
    ä¿å­˜è¯„ä¼°ç»“æœ
    
    Args:
        base_result: BaseRetrieverç»“æœ
        rerank_result: Rerankerç»“æœ
    """
    output_path = Path("tests/results/reranker_debug/full_evaluation.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # å‡†å¤‡æ•°æ®
    data = {
        'config': CONFIG,
        'base_retriever': base_result,
        'reranker': rerank_result,
        'comparison': {
            'recall_diff': rerank_result['recall_at_k'] - base_result['recall_at_k'],
            'recall_diff_pct': (rerank_result['recall_at_k'] - base_result['recall_at_k']) / base_result['recall_at_k'] * 100 if base_result['recall_at_k'] > 0 else 0,
            'time_ratio': rerank_result['avg_total_time_ms'] / base_result['avg_time_ms'] if base_result['avg_time_ms'] > 0 else 0
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹Rerankerå®Œæ•´è¯„ä¼°\n")
    
    try:
        # 1. åŠ è½½æµ‹è¯•é›†
        test_queries = load_test_queries(CONFIG['test_file'])
        
        # 2. åŠ è½½æ¨¡å‹
        retriever, reranker = load_models()
        
        # 3. è¯„ä¼°BaseRetriever
        base_result = evaluate_retriever(
            retriever,
            test_queries,
            top_k=CONFIG['top_k'],
            name="BaseRetriever"
        )
        
        # 4. è¯„ä¼°Reranker
        rerank_result = evaluate_reranker(
            retriever,
            reranker,
            test_queries,
            top_k=CONFIG['top_k']
        )
        
        # 5. å¯¹æ¯”åˆ†æ
        compare_results(base_result, rerank_result, CONFIG['top_k'])
        
        # 6. ä¿å­˜ç»“æœ
        save_results(base_result, rerank_result)
        
        # 7. æ€»ç»“
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… è¯„ä¼°å®Œæˆ!")
        logger.info(f"{'='*80}")
        
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()