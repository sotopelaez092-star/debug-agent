"""ç”¨Mockæ•°æ®æµ‹è¯•è¯„ä¼°å™¨ï¼ˆä¸ä¾èµ–æ•°æ®åº“ï¼‰"""
from src.evaluation.retrieval_eval import RetrievalEvaluator

print("=" * 60)
print("Mockæµ‹è¯•ï¼šéªŒè¯è¯„ä¼°å™¨åŠŸèƒ½")
print("=" * 60)

class MockRetriever:
    """æ¨¡æ‹Ÿæ£€ç´¢å™¨
    
    BaseRetriever: æ¨¡æ‹Ÿ60%å¬å›ç‡ï¼ŒMRR 0.5
    RerankerRetriever: æ¨¡æ‹Ÿ75%å¬å›ç‡ï¼ŒMRR 0.7
    """
    
    def __init__(self, name, performance="base"):
        self._name = name
        self.performance = performance
        self.__class__.__name__ = name
    
    def search(self, query, top_k=5):
        """è¿”å›æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ"""
        # BaseRetriever: ç®€å•é¡ºåº
        if self.performance == "base":
            return [1, 2, 3, 4, 5][:top_k]
        # RerankerRetriever: æ›´å¥½çš„æ’åº
        else:
            return [3, 1, 5, 2, 4][:top_k]

print("\n1ï¸âƒ£  åˆ›å»ºè¯„ä¼°å™¨...")
evaluator = RetrievalEvaluator()
print("âœ… è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")

print("\n2ï¸âƒ£  æµ‹è¯•å•ä¸ªæ–¹æ³•...")

# æµ‹è¯• Recall@K
print("  - æµ‹è¯• calculate_recall_at_k...")
result = evaluator.calculate_recall_at_k(
    retrieved_docs=[1, 2, 3, 4, 5],
    ground_truth=[1, 3, 5, 7, 9],
    k=3
)
print(f"    âœ… Recall@3 = {result['recall']:.1%} (æ‰¾åˆ° {result['found']}/{result['total']})")

# æµ‹è¯• MRR
print("  - æµ‹è¯• calculate_mrr...")
result = evaluator.calculate_mrr(
    retrieved_docs=[3, 2, 7, 10, 15],
    ground_truth=[2, 5, 10]
)
print(f"    âœ… MRR = {result['rr']:.3f} (ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£åœ¨ç¬¬{result['first_relevant_rank']}ä½)")

# æµ‹è¯•æ—¶é—´ç»Ÿè®¡
print("  - æµ‹è¯• calculate_avg_time...")
result = evaluator.calculate_avg_time([0.1, 0.2, 0.3, 0.4, 0.5])
print(f"    âœ… å¹³å‡æ—¶é—´ = {result['avg_time']:.3f}s (èŒƒå›´: {result['min_time']:.3f}s - {result['max_time']:.3f}s)")

print("\n3ï¸âƒ£  å‡†å¤‡æµ‹è¯•æ•°æ®...")
# 20ä¸ªæµ‹è¯•æ¡ˆä¾‹ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯
test_cases = [
    {'query': 'AttributeError: NoneType has no attribute get', 'ground_truth': [1, 3, 5]},
    {'query': 'TypeError: can only concatenate str not int', 'ground_truth': [2, 3, 4]},
    {'query': 'ValueError: invalid literal for int base 10', 'ground_truth': [1, 2, 5]},
    {'query': 'IndexError: list index out of range', 'ground_truth': [3, 4, 5]},
    {'query': 'KeyError: key not found in dictionary', 'ground_truth': [1, 4, 5]},
    {'query': 'NameError: name is not defined', 'ground_truth': [2, 4, 5]},
    {'query': 'ImportError: cannot import module', 'ground_truth': [1, 3, 4]},
    {'query': 'ZeroDivisionError: division by zero', 'ground_truth': [2, 3, 5]},
    {'query': 'FileNotFoundError: file does not exist', 'ground_truth': [1, 2, 3]},
    {'query': 'PermissionError: access denied', 'ground_truth': [3, 4, 5]},
    {'query': 'MemoryError: out of memory', 'ground_truth': [1, 3, 5]},
    {'query': 'RecursionError: maximum recursion depth', 'ground_truth': [2, 4, 5]},
    {'query': 'IndentationError: unexpected indent', 'ground_truth': [1, 2, 4]},
    {'query': 'SyntaxError: invalid syntax', 'ground_truth': [2, 3, 4]},
    {'query': 'StopIteration: iteration stopped', 'ground_truth': [1, 3, 4]},
    {'query': 'AssertionError: assertion failed', 'ground_truth': [2, 3, 5]},
    {'query': 'RuntimeError: runtime error occurred', 'ground_truth': [1, 4, 5]},
    {'query': 'NotImplementedError: method not implemented', 'ground_truth': [2, 3, 4]},
    {'query': 'UnicodeDecodeError: codec cannot decode', 'ground_truth': [1, 3, 5]},
    {'query': 'ConnectionError: connection failed', 'ground_truth': [2, 4, 5]},
]
print(f"âœ… å‡†å¤‡äº† {len(test_cases)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")

print("\n4ï¸âƒ£  å¯¹æ¯”ä¸¤ä¸ªæ£€ç´¢å™¨...")
base = MockRetriever("BaseRetriever", performance="base")
reranker = MockRetriever("RerankerRetriever", performance="reranker")

result = evaluator.compare_retrievers(
    retriever_a=base,
    retriever_b=reranker,
    test_cases=test_cases,
    k=5
)

print("\n" + "=" * 60)
print("ğŸ“Š å¯¹æ¯”ç»“æœ")
print("=" * 60)
print(f"\nğŸ“Œ BaseRetriever:")
print(f"   - Recall@5: {result['retriever_a']['recall@k']:.1%}")
print(f"   - MRR: {result['retriever_a']['mrr']:.3f}")
print(f"   - å¹³å‡æ—¶é—´: {result['retriever_a']['avg_time']:.3f}s")

print(f"\nğŸ“Œ RerankerRetriever:")
print(f"   - Recall@5: {result['retriever_b']['recall@k']:.1%}")
print(f"   - MRR: {result['retriever_b']['mrr']:.3f}")
print(f"   - å¹³å‡æ—¶é—´: {result['retriever_b']['avg_time']:.3f}s")

print(f"\nğŸ“ˆ æå‡:")
print(f"   - Recallæå‡: {result['comparison']['recall_improvement']:+.1%}")
print(f"   - MRRæå‡: {result['comparison']['mrr_improvement']:+.3f}")
print(f"   - æ—¶é—´å¢åŠ : {result['comparison']['time_overhead']:+.3f}s")

print("\n5ï¸âƒ£  ç”ŸæˆæŠ¥å‘Š...")
report = evaluator.generate_report(result, output_file='docs/week2_report_mock.md')
print("âœ… MockæŠ¥å‘Šå·²ä¿å­˜åˆ°: docs/week2_report_mock.md")

print("\n" + "=" * 60)
print("ğŸ‰ Mockæµ‹è¯•å®Œæˆï¼")
print("=" * 60)

print("\nğŸ“„ æŠ¥å‘Šé¢„è§ˆ:")
print("-" * 60)
print(report)