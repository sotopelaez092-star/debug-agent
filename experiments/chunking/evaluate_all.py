# experiments/chunking/evaluate_all.py
"""æ‰¹é‡è¯„ä¼°Chunkingç­–ç•¥"""
import sys
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

import json
import logging
from typing import Dict, List, Any
import chromadb

from src.rag.evaluator import ChunkingEvaluator
from src.rag.retriever import BaseRetriever
from src.rag.embedder import Embedder


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# å®šä¹‰6ä¸ªç­–ç•¥
STRATEGIES = {
    'S0': {'name': 'Full Document', 'vectorstore': 'chroma_s0'},
    'S1': {'name': 'Semantic', 'vectorstore': 'chroma_s1'},
    'S2': {'name': 'Answer-Only', 'vectorstore': 'chroma_s2'},
    'S3': {'name': 'Title+Answer', 'vectorstore': 'chroma_s3'},
    'S4': {'name': 'Fixed-200', 'vectorstore': 'chroma_s4'},
    'S5': {'name': 'Fixed-300', 'vectorstore': 'chroma_s5'},
}

# æ•°æ®è·¯å¾„
DATA_DIR = Path(__file__).parent.parent.parent / 'data'
QUERIES_PATH = DATA_DIR / 'test_cases' / 'test_queries_realistic.json'
GT_PATH = DATA_DIR / 'evaluation' / 'llm_annotated_gt.json'
VECTORSTORE_DIR = DATA_DIR / 'vectorstore'




def load_queries() -> List[Dict[str, str]]:
    """
    åŠ è½½æµ‹è¯•queries
    
    Returns:
        queriesåˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{'id': 'test-001', 'text': 'query text'}, ...]
    """
    if not QUERIES_PATH.exists():
        logger.error(f"æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {QUERIES_PATH}")
        raise FileNotFoundError(f"æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {QUERIES_PATH}")
    
    try:
        with open(QUERIES_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
            raw_queries = data.get('queries', [])
            
            # âœ… æ ¼å¼è½¬æ¢ï¼šquery_id -> id, query -> text
            queries = []
            for q in raw_queries:
                queries.append({
                    'id': q['query_id'],      # â† è½¬æ¢å­—æ®µå
                    'text': q['query']        # â† è½¬æ¢å­—æ®µå
                })
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(queries)} ä¸ªæŸ¥è¯¢")
            return queries
            
    except json.JSONDecodeError:
        logger.error(f"æŸ¥è¯¢æ–‡ä»¶ {QUERIES_PATH} æ ¼å¼é”™è¯¯")
        raise ValueError(f"æŸ¥è¯¢æ–‡ä»¶ {QUERIES_PATH} æ ¼å¼é”™è¯¯")

def load_ground_truth() -> Dict[str, List[str]]:
    """
    åŠ è½½ground truth
    
    Returns:
        ground truthå­—å…¸ï¼Œæ ¼å¼ï¼š{'test-001': ['doc-1', 'doc-2'], ...}
    """
    # æ£€æŸ¥ GT_PATH æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    # è¯»å– JSON æ–‡ä»¶
    # è¿”å› ground truth å­—å…¸
    if not GT_PATH.exists():
        logger.error(f"ground truth æ–‡ä»¶ä¸å­˜åœ¨: {GT_PATH}")
        raise FileNotFoundError(f"ground truth æ–‡ä»¶ä¸å­˜åœ¨: {GT_PATH}")
    
    try:
        with open(GT_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
            annotations = data.get('annotations', [])
            
            # è½¬æ¢æ ¼å¼
            ground_truth = {}
            for anno in annotations:
                query_id = anno['query_id']
                relevant_docs = anno.get('relevant_docs', [])
                ground_truth[query_id] = relevant_docs
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(ground_truth)} ä¸ªground truth")
            return ground_truth
    except json.JSONDecodeError:
        logger.error(f"ground truth æ–‡ä»¶ {GT_PATH} æ ¼å¼é”™è¯¯")
        raise ValueError(f"ground truth æ–‡ä»¶ {GT_PATH} æ ¼å¼é”™è¯¯")

def evaluate_strategy(
    strategy_id: str,
    config: Dict[str, str],
    queries: List[Dict[str, str]],
    ground_truth: Dict[str, List[str]]
) -> Dict[str, Any]:
    """
    è¯„ä¼°å•ä¸ªchunkingç­–ç•¥
    
    Args:
        strategy_id: ç­–ç•¥IDï¼ˆå¦‚ 'S0'ï¼‰
        config: ç­–ç•¥é…ç½® {'name': 'Full Document', 'vectorstore': 'chroma_s0'}
        queries: æµ‹è¯•queries
        ground_truth: ground truth
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
        
    Raises:
        FileNotFoundError: å½“å‘é‡åº“ä¸å­˜åœ¨æ—¶
        Exception: å…¶ä»–è¯„ä¼°é”™è¯¯
    """
    strategy_name = config['name']
    vectorstore_name = config['vectorstore']
    
    logger.info(f"\n{'='*60}")
    logger.info(f"è¯„ä¼°ç­–ç•¥: {strategy_id} - {strategy_name}")
    logger.info(f"å‘é‡åº“: {vectorstore_name}")
    logger.info(f"{'='*60}")
    
    try:
        # 1. åˆå§‹åŒ–ChromaDB client
        vectorstore_path = VECTORSTORE_DIR / vectorstore_name
        if not vectorstore_path.exists():
            raise FileNotFoundError(f"å‘é‡åº“ä¸å­˜åœ¨: {vectorstore_path}")
        
        logger.info(f"åŠ è½½å‘é‡åº“: {vectorstore_path}")
        client = chromadb.PersistentClient(path=str(vectorstore_path))
        
        # 2. è·å–collection
        collection = client.get_collection(name="stackoverflow_kb")
        doc_count = collection.count()
        logger.info(f"CollectionåŒ…å« {doc_count} ä¸ªæ–‡æ¡£")
        
        # 3. åˆ›å»ºEmbedderå®ä¾‹
        embedder = Embedder(model_name="BAAI/bge-small-en-v1.5")
        logger.info("Embedderå·²åˆ›å»º")

        # 4. åˆ›å»ºBaseRetriever
        retriever = BaseRetriever(
            collection=collection,
            embedding_function=embedder,  # âœ… ç›´æ¥ä¼ å…¥embedder
            min_similarity=0.5,
            recall_factor=4
        )
        logger.info("Retrieverå·²åˆå§‹åŒ–")
        
        # 5. åˆ›å»ºChunkingEvaluator
        evaluator = ChunkingEvaluator(retriever=retriever)
        
        # 6. è°ƒç”¨evaluate()
        logger.info("å¼€å§‹è¯„ä¼°...")
        results = evaluator.evaluate(
            queries=queries,
            ground_truth=ground_truth,
            k_values=[1, 3, 5, 10]
        )
        
        # 7. æ·»åŠ ç­–ç•¥ä¿¡æ¯åˆ°ç»“æœä¸­
        results['strategy_id'] = strategy_id
        results['strategy_name'] = strategy_name
        results['vectorstore'] = vectorstore_name
        results['doc_count'] = doc_count
        
        # 8. æ‰“å°ç»“æœæ‘˜è¦
        logger.info(f"\n{'='*60}")
        logger.info(f"ç­–ç•¥ {strategy_id} è¯„ä¼°å®Œæˆï¼")
        logger.info(f"  Recall@1:  {results['recall'].get(1, 0):.3f}")
        logger.info(f"  Recall@3:  {results['recall'].get(3, 0):.3f}")
        logger.info(f"  Recall@5:  {results['recall'].get(5, 0):.3f}")
        logger.info(f"  Recall@10: {results['recall'].get(10, 0):.3f}")
        logger.info(f"  MRR:       {results['mrr']:.3f}")
        logger.info(f"  å¹³å‡æ—¶é—´:   {results['avg_retrieval_time']:.3f}s")
        logger.info(f"  å¤±è´¥ç‡:     {results['failure_rate']:.2%}")
        logger.info(f"{'='*60}\n")
        
        return results
        
    except FileNotFoundError as e:
        logger.error(f"å‘é‡åº“æ–‡ä»¶é”™è¯¯: {e}")
        raise
    except Exception as e:
        logger.error(f"è¯„ä¼°ç­–ç•¥ {strategy_id} å¤±è´¥: {e}", exc_info=True)
        raise
def print_comparison_table(all_results: Dict[str, Dict]):
    """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "=" * 100)
    print("ğŸ“Š Chunkingç­–ç•¥å¯¹æ¯”")
    print("=" * 100)
    
    # è¡¨å¤´
    header = f"{'ç­–ç•¥':<8} {'åç§°':<20} {'R@1':<8} {'R@3':<8} {'R@5':<8} {'R@10':<8} {'MRR':<8} {'é€Ÿåº¦(ms)':<10} {'å¤±è´¥ç‡':<8}"
    print(header)
    print("-" * 100)
    
    # æ¯ä¸ªç­–ç•¥çš„ç»“æœ
    for strategy_id in sorted(all_results.keys()):
        results = all_results[strategy_id]
        
        row = (
            f"{strategy_id:<8} "
            f"{results['strategy_name']:<20} "
            f"{results['recall'].get(1, 0):.3f}    "
            f"{results['recall'].get(3, 0):.3f}    "
            f"{results['recall'].get(5, 0):.3f}    "
            f"{results['recall'].get(10, 0):.3f}    "
            f"{results['mrr']:.3f}    "
            f"{results['avg_retrieval_time']*1000:.1f}      "
            f"{results['failure_rate']:.2%}"
        )
        print(row)
    
    print("=" * 100)
    
    # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
    best_recall5 = max(all_results.items(), key=lambda x: x[1]['recall'].get(5, 0))
    best_mrr = max(all_results.items(), key=lambda x: x[1]['mrr'])
    best_speed = min(all_results.items(), key=lambda x: x[1]['avg_retrieval_time'])
    
    print(f"\nğŸ† æœ€ä½³ç­–ç•¥:")
    print(f"  - Recall@5: {best_recall5[0]} ({best_recall5[1]['recall'].get(5, 0):.3f})")
    print(f"  - MRR:      {best_mrr[0]} ({best_mrr[1]['mrr']:.3f})")
    print(f"  - é€Ÿåº¦:      {best_speed[0]} ({best_speed[1]['avg_retrieval_time']*1000:.1f}ms)")
    print()


def save_results(all_results: Dict[str, Dict]):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    results_dir = Path(__file__).parent / 'results'
    results_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜JSON
    json_path = results_dir / 'evaluation_results.json'
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    logger.info(f"ç»“æœå·²ä¿å­˜: {json_path}")
    
    # ä¿å­˜CSVï¼ˆæ–¹ä¾¿Excelæ‰“å¼€ï¼‰
    csv_path = results_dir / 'evaluation_results.csv'
    with open(csv_path, 'w', encoding='utf-8') as f:
        # CSVè¡¨å¤´
        f.write("Strategy,Name,Recall@1,Recall@3,Recall@5,Recall@10,MRR,AvgTime(ms),FailureRate\n")
        
        # æ¯è¡Œæ•°æ®
        for strategy_id in sorted(all_results.keys()):
            r = all_results[strategy_id]
            f.write(
                f"{strategy_id},"
                f"{r['strategy_name']},"
                f"{r['recall'].get(1, 0):.3f},"
                f"{r['recall'].get(3, 0):.3f},"
                f"{r['recall'].get(5, 0):.3f},"
                f"{r['recall'].get(10, 0):.3f},"
                f"{r['mrr']:.3f},"
                f"{r['avg_retrieval_time']*1000:.1f},"
                f"{r['failure_rate']:.2%}\n"
            )
    logger.info(f"CSVå·²ä¿å­˜: {csv_path}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # 1. åŠ è½½æ•°æ®
        logger.info("=" * 80)
        logger.info("å¼€å§‹æ‰¹é‡è¯„ä¼°Chunkingç­–ç•¥")
        logger.info("=" * 80)
        
        queries = load_queries()
        ground_truth = load_ground_truth()
        
        logger.info(f"\nåŠ è½½å®Œæˆ:")
        logger.info(f"  - Queries: {len(queries)} ä¸ª")
        logger.info(f"  - Ground Truth: {len(ground_truth)} ä¸ª")
        
        # 2. è¯„ä¼°æ‰€æœ‰ç­–ç•¥
        all_results = {}
        
        for strategy_id, config in STRATEGIES.items():
            try:
                results = evaluate_strategy(
                    strategy_id=strategy_id,
                    config=config,
                    queries=queries,
                    ground_truth=ground_truth
                )
                all_results[strategy_id] = results
            except Exception as e:
                logger.error(f"è·³è¿‡ç­–ç•¥ {strategy_id}: {e}")
                continue
        
        # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        if all_results:
            print_comparison_table(all_results)
            save_results(all_results)
        else:
            logger.error("æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„ç­–ç•¥ï¼")
            return
        
        logger.info("\n" + "=" * 80)
        logger.info("æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"ä¸»å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == '__main__':
    main()