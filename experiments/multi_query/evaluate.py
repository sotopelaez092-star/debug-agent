# experiments/multi_query/evaluate.py
"""
Multi-Queryæ£€ç´¢å™¨è¯„ä¼°è„šæœ¬

å¯¹æ¯”ï¼š
- Baseline: Queryæ”¹å†™ + BaseRetriever
- Multi-Query: MultiQueryRetriever

è¯„ä¼°æŒ‡æ ‡ï¼š
- Recall@5
- Recall@10
- æ£€ç´¢æ—¶é—´
"""

import json
import logging
import sys
import time
from pathlib import Path
from typing import Dict, List

import chromadb

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.rag.retriever import BaseRetriever
from src.rag.embedder import Embedder
from src.rag.query_rewriter import QueryRewriter
from src.rag.multi_query_retriever import MultiQueryRetriever

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_queries(file_path: str) -> List[Dict[str, str]]:
    """åŠ è½½æµ‹è¯•æŸ¥è¯¢"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    queries = []
    for item in data.get('queries', []):
        query_id = item.get('query_id')
        query_text = item.get('query')
        if query_id and query_text:
            queries.append({'id': query_id, 'text': query_text})
    
    logger.info(f"åŠ è½½{len(queries)}ä¸ªæµ‹è¯•æŸ¥è¯¢")
    return queries


def load_ground_truth(file_path: str) -> Dict[str, List[str]]:
    """åŠ è½½Ground Truth"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    ground_truth = {}
    for anno in data.get('annotations', []):
        query_id = anno.get('query_id')
        relevant_docs = anno.get('relevant_docs', [])
        if query_id and relevant_docs:
            ground_truth[query_id] = relevant_docs
    
    logger.info(f"åŠ è½½{len(ground_truth)}ä¸ªground truth")
    return ground_truth


def extract_base_doc_id(doc_id: str) -> str:
    """æå–åŸºç¡€æ–‡æ¡£ID"""
    if '_chunk_' in doc_id:
        return doc_id.split('_chunk_')[0]
    return doc_id


def calculate_recall(
    retrieved_ids: List[str], 
    ground_truth_ids: List[str], 
    k: int
) -> float:
    """è®¡ç®—Recall@k"""
    if not ground_truth_ids:
        return 0.0
    
    top_k_ids = retrieved_ids[:k]
    retrieved_base = set(extract_base_doc_id(id_) for id_ in top_k_ids)
    gt_base = set(extract_base_doc_id(id_) for id_ in ground_truth_ids)
    hits = retrieved_base & gt_base
    
    return len(hits) / len(gt_base)


def main():
    """ä¸»å‡½æ•°"""
    try:
        # é…ç½®
        QUERIES_FILE = "data/test_cases/test_queries_realistic.json"
        GT_FILE = "data/evaluation/llm_annotated_gt.json"
        VECTORSTORE_PATH = "data/vectorstore/chroma_s1"
        OUTPUT_FILE = "experiments/multi_query/results/evaluation_results.json"
        
        logger.info("=" * 60)
        logger.info("Multi-Queryæ£€ç´¢å™¨è¯„ä¼°")
        logger.info("=" * 60)
        
        # Step 1: åŠ è½½æ•°æ®
        logger.info("\n[Step 1/4] åŠ è½½æ•°æ®...")
        queries = load_queries(QUERIES_FILE)
        ground_truth = load_ground_truth(GT_FILE)
        
        # Step 2: åˆå§‹åŒ–ç»„ä»¶
        logger.info("\n[Step 2/4] åˆå§‹åŒ–ç»„ä»¶...")
        embedder = Embedder("BAAI/bge-small-en-v1.5")
        client = chromadb.PersistentClient(path=VECTORSTORE_PATH)
        collection = client.get_collection(name="stackoverflow_kb")
        
        # BaseRetriever
        base_retriever = BaseRetriever(
            collection, 
            embedder, 
            min_similarity=0.5, 
            recall_factor=4
        )
        
        # QueryRewriter
        rewriter = QueryRewriter()
        
        # MultiQueryRetriever
        multi_query_retriever = MultiQueryRetriever(
            base_retriever=base_retriever,
            num_queries=3,
            top_k_per_query=10,
            temperature=0.7
        )
        
        logger.info("ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        
        # Step 3: è¯„ä¼°
        logger.info("\n[Step 3/4] å¼€å§‹è¯„ä¼°...")
        
        baseline_results = []
        multiquery_results = []
        
        for idx, query in enumerate(queries, 1):
            query_id = query['id']
            
            if query_id not in ground_truth:
                continue
            
            gt_ids = ground_truth[query_id]
            original_query = query['text']
            
            # Baseline: Queryæ”¹å†™ + Baseæ£€ç´¢
            rewritten = rewriter.rewrite(original_query)
            
            start_time = time.time()
            base_docs = base_retriever.search(rewritten, top_k=10)
            base_time = time.time() - start_time
            
            base_ids = [d['id'] for d in base_docs]
            base_r5 = calculate_recall(base_ids, gt_ids, 5)
            base_r10 = calculate_recall(base_ids, gt_ids, 10)
            
            # Multi-Query
            start_time = time.time()
            mq_docs = multi_query_retriever.search(original_query, top_k=10)
            mq_time = time.time() - start_time
            
            mq_ids = [d['id'] for d in mq_docs]
            mq_r5 = calculate_recall(mq_ids, gt_ids, 5)
            mq_r10 = calculate_recall(mq_ids, gt_ids, 10)
            
            baseline_results.append({
                'query_id': query_id,
                'recall_5': base_r5,
                'recall_10': base_r10,
                'time': base_time
            })
            
            multiquery_results.append({
                'query_id': query_id,
                'recall_5': mq_r5,
                'recall_10': mq_r10,
                'time': mq_time,
                'improvement_r5': mq_r5 - base_r5,
                'improvement_r10': mq_r10 - base_r10
            })
            
            logger.info(
                f"[{idx}/{len(queries)}] {query_id}:\n"
                f"  Baseline: R@5={base_r5:.2%}, R@10={base_r10:.2%}, "
                f"Time={base_time:.2f}s\n"
                f"  Multi-Q:  R@5={mq_r5:.2%}, R@10={mq_r10:.2%}, "
                f"Time={mq_time:.2f}s\n"
                f"  Change:   Î”R@5={mq_r5-base_r5:+.2%}, "
                f"Î”R@10={mq_r10-base_r10:+.2%}"
            )
        
        # Step 4: æ±‡æ€»ç»“æœ
        logger.info("\n[Step 4/4] æ±‡æ€»ç»“æœ...")
        
        n = len(baseline_results)
        
        avg_base_r5 = sum(r['recall_5'] for r in baseline_results) / n
        avg_base_r10 = sum(r['recall_10'] for r in baseline_results) / n
        avg_base_time = sum(r['time'] for r in baseline_results) / n
        
        avg_mq_r5 = sum(r['recall_5'] for r in multiquery_results) / n
        avg_mq_r10 = sum(r['recall_10'] for r in multiquery_results) / n
        avg_mq_time = sum(r['time'] for r in multiquery_results) / n
        
        imp_r5 = avg_mq_r5 - avg_base_r5
        imp_r10 = avg_mq_r10 - avg_base_r10
        imp_r5_pct = (imp_r5 / avg_base_r5 * 100) if avg_base_r5 > 0 else 0
        imp_r10_pct = (imp_r10 / avg_base_r10 * 100) if avg_base_r10 > 0 else 0
        
        # ç»Ÿè®¡æå‡æƒ…å†µ
        better_r5 = sum(1 for r in multiquery_results if r['improvement_r5'] > 0)
        better_r10 = sum(1 for r in multiquery_results if r['improvement_r10'] > 0)
        worse_r5 = sum(1 for r in multiquery_results if r['improvement_r5'] < 0)
        worse_r10 = sum(1 for r in multiquery_results if r['improvement_r10'] < 0)
        same_r5 = n - better_r5 - worse_r5
        same_r10 = n - better_r10 - worse_r10
        
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯„ä¼°ç»“æœ")
        print("=" * 60)
        print(f"æµ‹è¯•æŸ¥è¯¢æ•°: {n}")
        
        print(f"\nã€Baseline - Queryæ”¹å†™ + Baseã€‘")
        print(f"  å¹³å‡ Recall@5:  {avg_base_r5:.2%}")
        print(f"  å¹³å‡ Recall@10: {avg_base_r10:.2%}")
        print(f"  å¹³å‡æ£€ç´¢æ—¶é—´:   {avg_base_time:.3f}s")
        
        print(f"\nã€Multi-Queryã€‘")
        print(f"  å¹³å‡ Recall@5:  {avg_mq_r5:.2%}")
        print(f"  å¹³å‡ Recall@10: {avg_mq_r10:.2%}")
        print(f"  å¹³å‡æ£€ç´¢æ—¶é—´:   {avg_mq_time:.3f}s")
        
        print(f"\nã€æå‡æƒ…å†µã€‘")
        print(f"  Recall@5:")
        print(f"    ç»å¯¹æå‡: {imp_r5:+.2%}")
        print(f"    ç›¸å¯¹æå‡: {imp_r5_pct:+.1f}%")
        print(f"    æ›´å¥½: {better_r5}ä¸ª ({better_r5/n*100:.1f}%)")
        print(f"    ç›¸åŒ: {same_r5}ä¸ª ({same_r5/n*100:.1f}%)")
        print(f"    æ›´å·®: {worse_r5}ä¸ª ({worse_r5/n*100:.1f}%)")
        
        print(f"  Recall@10:")
        print(f"    ç»å¯¹æå‡: {imp_r10:+.2%}")
        print(f"    ç›¸å¯¹æå‡: {imp_r10_pct:+.1f}%")
        print(f"    æ›´å¥½: {better_r10}ä¸ª ({better_r10/n*100:.1f}%)")
        print(f"    ç›¸åŒ: {same_r10}ä¸ª ({same_r10/n*100:.1f}%)")
        print(f"    æ›´å·®: {worse_r10}ä¸ª ({worse_r10/n*100:.1f}%)")
        
        print(f"  æ£€ç´¢æ—¶é—´:")
        print(f"    å¢åŠ : {avg_mq_time - avg_base_time:+.3f}s")
        print(f"    ç›¸å¯¹å¢åŠ : {(avg_mq_time/avg_base_time-1)*100:+.1f}%")
        print("=" * 60)
        
        # æ‰“å°æå‡æœ€å¤§çš„æ¡ˆä¾‹
        print("\nğŸ” Recall@10æå‡æœ€å¤§çš„Top5æŸ¥è¯¢:")
        sorted_r10 = sorted(
            multiquery_results,
            key=lambda x: x['improvement_r10'],
            reverse=True
        )
        for r in sorted_r10[:5]:
            base_r = next(
                b['recall_10'] for b in baseline_results 
                if b['query_id'] == r['query_id']
            )
            print(
                f"  {r['query_id']}: {base_r:.2%} â†’ {r['recall_10']:.2%} "
                f"({r['improvement_r10']:+.2%})"
            )
        
        # ä¿å­˜ç»“æœ
        Path(OUTPUT_FILE).parent.mkdir(parents=True, exist_ok=True)
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                'summary': {
                    'total': n,
                    'baseline': {
                        'recall_5': avg_base_r5,
                        'recall_10': avg_base_r10,
                        'avg_time': avg_base_time
                    },
                    'multiquery': {
                        'recall_5': avg_mq_r5,
                        'recall_10': avg_mq_r10,
                        'avg_time': avg_mq_time
                    },
                    'improvement': {
                        'recall_5_abs': imp_r5,
                        'recall_5_rel': imp_r5_pct,
                        'recall_10_abs': imp_r10,
                        'recall_10_rel': imp_r10_pct,
                        'better_r5': better_r5,
                        'better_r10': better_r10,
                        'worse_r5': worse_r5,
                        'worse_r10': worse_r10
                    }
                },
                'baseline_results': baseline_results,
                'multiquery_results': multiquery_results
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nâœ… å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_FILE}")
        
        # å†³ç­–å»ºè®®
        print("\nğŸ’¡ å†³ç­–å»ºè®®:")
        if imp_r10 >= 0.05:  # æå‡â‰¥5%
            print("  âœ… Multi-Queryæ•ˆæœæ˜¾è‘—ï¼Œå»ºè®®é‡‡ç”¨ï¼")
        elif 0.02 <= imp_r10 < 0.05:  # æå‡2-5%
            print("  âš ï¸ Multi-Queryæœ‰ä¸€å®šæ•ˆæœï¼Œå¯ä»¥è€ƒè™‘é‡‡ç”¨")
            print("     ä½†éœ€è¦æƒè¡¡æ£€ç´¢æ—¶é—´å¢åŠ çš„æˆæœ¬")
        else:  # æå‡<2%
            print("  âŒ Multi-Queryæå‡ä¸æ˜æ˜¾ï¼Œä¸å»ºè®®é‡‡ç”¨")
            print("     å½“å‰Queryæ”¹å†™ + Baseå·²ç»è¶³å¤Ÿå¥½")
        
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
