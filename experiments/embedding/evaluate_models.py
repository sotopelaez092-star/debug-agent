#!/usr/bin/env python3
"""
è¯„ä¼°ä¸åŒEmbeddingæ¨¡å‹çš„æ€§èƒ½

åŠŸèƒ½ï¼š
1. å¯¹æ¯”4ä¸ªembeddingæ¨¡å‹
2. ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•querieså’Œground truth
3. ç”Ÿæˆå®Œæ•´çš„å¯¹æ¯”æŠ¥å‘Š

ç”¨æ³•:
    python experiments/embedding/evaluate_models.py
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import json
import logging
import os
import csv
from typing import List, Dict, Any
import time

import chromadb
from src.rag.embedder import Embedder
from src.rag.retriever import BaseRetriever
from src.rag.evaluator import ChunkingEvaluator

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# âœ… å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹
EMBEDDING_MODELS = {
    'M1': {
        'name': 'bge-small-en',
        'model_name': 'BAAI/bge-small-en-v1.5',
        'vectorstore': 'data/vectorstore/chroma_s1',  # ç”¨åŸå§‹çš„S1
        'dimension': 384,
        'description': 'å½“å‰åŸºçº¿'
    },
    'M2': {
        'name': 'bge-base-en',
        'model_name': 'BAAI/bge-base-en-v1.5',
        'vectorstore': 'data/vectorstore/embed_m2',
        'dimension': 768,
        'description': 'ä¸­ç­‰æ¨¡å‹'
    },
    'M3': {
        'name': 'bge-m3',
        'model_name': 'BAAI/bge-m3',
        'vectorstore': 'data/vectorstore/embed_m3',
        'dimension': 1024,
        'description': 'å¤šè¯­è¨€å¤§æ¨¡å‹'
    },
    'M4': {
        'name': 'all-MiniLM',
        'model_name': 'sentence-transformers/all-MiniLM-L6-v2',
        'vectorstore': 'data/vectorstore/embed_m4',
        'dimension': 384,
        'description': 'è½»é‡çº§åŸºçº¿'
    }
}


def load_queries() -> List[Dict[str, str]]:
    """
    åŠ è½½æµ‹è¯•queries
    
    Returns:
        queriesåˆ—è¡¨ï¼Œæ ¼å¼: [{'id': 'test-001', 'text': 'query'}, ...]
        
    Raises:
        FileNotFoundError: å½“æ–‡ä»¶ä¸å­˜åœ¨æ—¶
        ValueError: å½“æ•°æ®æ ¼å¼é”™è¯¯æ—¶
    """
    query_file = 'data/test_cases/test_queries_realistic.json'
    
    if not Path(query_file).exists():
        raise FileNotFoundError(f"æµ‹è¯•queriesæ–‡ä»¶æœªæ‰¾åˆ°: {query_file}")
    
    try:
        with open(query_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        raw_queries = data.get('queries', [])
        
        if not raw_queries:
            raise ValueError("queriesåˆ—è¡¨ä¸ºç©º")
        
        # âœ… æ ¼å¼è½¬æ¢ï¼šquery_id -> id, query -> text
        queries = []
        for q in raw_queries:
            if 'query_id' not in q or 'query' not in q:
                logger.warning(f"è·³è¿‡æ ¼å¼é”™è¯¯çš„query: {q}")
                continue
            
            queries.append({
                'id': q['query_id'],
                'text': q['query']
            })
        
        return queries
        
    except json.JSONDecodeError as e:
        logger.error(f"JSONè§£æå¤±è´¥: {e}")
        raise
    except Exception as e:
        logger.error(f"åŠ è½½querieså¤±è´¥: {e}", exc_info=True)
        raise


def load_ground_truth() -> Dict[str, List[str]]:
    """
    åŠ è½½ground truth
    
    Returns:
        ground truthå­—å…¸ï¼Œæ ¼å¼: {'test-001': ['doc-id1', 'doc-id2'], ...}
        
    Raises:
        FileNotFoundError: å½“æ–‡ä»¶ä¸å­˜åœ¨æ—¶
        ValueError: å½“æ•°æ®æ ¼å¼é”™è¯¯æ—¶
    """
    gt_file = 'data/evaluation/llm_annotated_gt.json'
    
    if not Path(gt_file).exists():
        raise FileNotFoundError(f"ground truthæ–‡ä»¶æœªæ‰¾åˆ°: {gt_file}")
    
    try:
        with open(gt_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        annotations = data.get('annotations', [])
        
        if not annotations:
            raise ValueError("annotationsåˆ—è¡¨ä¸ºç©º")
        
        # âœ… è½¬æ¢æ ¼å¼: ä»annotationsåˆ—è¡¨åˆ°å­—å…¸
        gt_dict = {}
        for ann in annotations:
            if 'query_id' not in ann:
                logger.warning(f"è·³è¿‡ç¼ºå°‘query_idçš„annotation: {ann}")
                continue
            
            query_id = ann['query_id']
            # âœ… å…³é”®ä¿®å¤ï¼šå­—æ®µåæ˜¯relevant_docsï¼Œä¸æ˜¯doc_ids
            relevant_docs = ann.get('relevant_docs', [])
            gt_dict[query_id] = relevant_docs
        
        return gt_dict
        
    except json.JSONDecodeError as e:
        logger.error(f"JSONè§£æå¤±è´¥: {e}")
        raise
    except Exception as e:
        logger.error(f"åŠ è½½ground truthå¤±è´¥: {e}", exc_info=True)
        raise


def evaluate_model(
    model_id: str,
    config: Dict[str, Any],
    queries: List[Dict[str, str]],
    ground_truth: Dict[str, List[str]]
) -> Dict[str, Any]:
    """
    è¯„ä¼°å•ä¸ªembeddingæ¨¡å‹
    
    Args:
        model_id: æ¨¡å‹ID (M1, M2, M3, M4)
        config: æ¨¡å‹é…ç½®
        queries: æµ‹è¯•queries
        ground_truth: ground truthæ•°æ®
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
        
    Raises:
        Exception: å½“è¯„ä¼°å¤±è´¥æ—¶
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"è¯„ä¼°æ¨¡å‹: {model_id} - {config['name']}")
    logger.info(f"æè¿°: {config['description']}")
    logger.info(f"ç»´åº¦: {config['dimension']}")
    logger.info(f"å‘é‡åº“: {config['vectorstore']}")
    logger.info(f"{'='*80}\n")
    
    try:
        # 1. åˆ›å»ºEmbedder
        # âœ… å…³é”®ä¿®å¤ï¼šEmbedderåªæ¥å—model_nameå‚æ•°
        logger.info(f"  ğŸ¤– åˆå§‹åŒ–Embedder...")
        embedder = Embedder(model_name=config['model_name'])
        
        # 2. è¿æ¥ChromaDB
        # âœ… å…³é”®ä¿®å¤ï¼šcollectionåç§°æ˜¯stackoverflow_kb
        logger.info(f"  ğŸ’¾ è¿æ¥ChromaDB...")
        vectorstore_path = Path(config['vectorstore'])
        if not vectorstore_path.exists():
            raise FileNotFoundError(f"å‘é‡åº“ä¸å­˜åœ¨: {config['vectorstore']}")
        
        client = chromadb.PersistentClient(path=config['vectorstore'])
        collection = client.get_collection(name="stackoverflow_kb")
        
        logger.info(f"  â„¹ï¸  å‘é‡åº“æ–‡æ¡£æ•°: {collection.count()}")
        
        # 3. åˆ›å»ºBaseRetriever
        logger.info(f"  ğŸ” åˆ›å»ºBaseRetriever...")
        retriever = BaseRetriever(
            collection=collection,
            embedding_function=embedder
        )
        
        # 4. åˆ›å»ºChunkingEvaluator
        # âœ… å…³é”®ä¿®å¤ï¼šChunkingEvaluatoråˆå§‹åŒ–åªæ¥å—retriever
        logger.info(f"  ğŸ“Š åˆ›å»ºEvaluator...")
        evaluator = ChunkingEvaluator(retriever=retriever)
        
        # 5. è¿è¡Œè¯„ä¼°
        # âœ… å…³é”®ä¿®å¤ï¼ševaluateéœ€è¦querieså’Œground_truthä¸¤ä¸ªå‚æ•°
        logger.info(f"  ğŸš€ å¼€å§‹è¯„ä¼°...")
        start_time = time.time()
        results = evaluator.evaluate(queries, ground_truth)
        elapsed_time = time.time() - start_time
        
        logger.info(f"  âœ… è¯„ä¼°å®Œæˆï¼è€—æ—¶: {elapsed_time:.2f}ç§’")
        
        # 6. æ·»åŠ æ¨¡å‹ä¿¡æ¯åˆ°ç»“æœ
        results['model_id'] = model_id
        results['model_name'] = config['name']
        results['model_full_name'] = config['model_name']
        results['dimension'] = config['dimension']
        results['description'] = config['description']
        results['total_eval_time'] = elapsed_time
        
        return results
        
    except Exception as e:
        logger.error(f"è¯„ä¼°{model_id}å¤±è´¥: {e}", exc_info=True)
        raise


def save_results(all_results: List[Dict], output_dir: str = "experiments/embedding/results"):
    """
    ä¿å­˜è¯„ä¼°ç»“æœ
    
    Args:
        all_results: æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
        
    Raises:
        Exception: å½“ä¿å­˜å¤±è´¥æ—¶
    """
    try:
        # 1. åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # 2. ä¿å­˜JSONæ ¼å¼
        json_path = Path(output_dir) / "evaluation_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        logger.info(f"  âœ… å·²ä¿å­˜JSONç»“æœ: {json_path}")
        
        # 3. ä¿å­˜CSVæ ¼å¼ï¼ˆæ–¹ä¾¿Excelæ‰“å¼€ï¼‰
        csv_path = Path(output_dir) / "evaluation_results.csv"
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            writer.writerow([
                'Model_ID',
                'Model_Name',
                'Description',
                'Dimension',
                'Recall@1',
                'Recall@3',
                'Recall@5',
                'Recall@10',
                'MRR',
                'Avg_Time(ms)',
                'Success_Rate',
                'Total_Queries',
                'Successful_Queries',
                'Failed_Queries'
            ])
            
            # å†™å…¥æ¯è¡Œæ•°æ®
            for result in all_results:
                # æ ¹æ®ChunkingEvaluatorçš„è¿”å›æ ¼å¼è®¿é—®å­—æ®µ
                recall = result['recall']
                
                writer.writerow([
                    result['model_id'],
                    result['model_name'],
                    result['description'],
                    result['dimension'],
                    f"{recall.get(1, 0):.2%}",
                    f"{recall.get(3, 0):.2%}",
                    f"{recall.get(5, 0):.2%}",
                    f"{recall.get(10, 0):.2%}",
                    f"{result['mrr']:.4f}",
                    f"{result['avg_retrieval_time'] * 1000:.1f}",
                    f"{1 - result['failure_rate']:.2%}",
                    result['total_queries'],
                    result['successful_queries'],
                    result['failed_queries']
                ])
        
        logger.info(f"  âœ… å·²ä¿å­˜CSVç»“æœ: {csv_path}")
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}", exc_info=True)
        raise


def print_comparison_table(all_results: List[Dict]):
    """
    æ‰“å°å¯¹æ¯”è¡¨æ ¼
    
    Args:
        all_results: æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°ç»“æœ
    """
    # è¡¨å¤´
    print(f"\n{'æ¨¡å‹':<12} {'ç»´åº¦':<8} {'Recall@1':<10} {'Recall@3':<10} {'Recall@5':<10} {'Recall@10':<11} {'MRR':<8} {'é€Ÿåº¦(ms)':<10}")
    print("=" * 95)
    
    # æ¯è¡Œæ•°æ®
    for result in all_results:
        model = result['model_name']
        dim = result['dimension']
        recall = result['recall']
        mrr = result['mrr']
        speed = result['avg_retrieval_time'] * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        print(
            f"{model:<12} "
            f"{dim:<8} "
            f"{recall.get(1, 0):<10.1%} "
            f"{recall.get(3, 0):<10.1%} "
            f"{recall.get(5, 0):<10.1%} "
            f"{recall.get(10, 0):<11.1%} "
            f"{mrr:<8.3f} "
            f"{speed:<10.1f}"
        )
    
    print("=" * 95)
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_recall5 = max(all_results, key=lambda x: x['recall'].get(5, 0))
    fastest = min(all_results, key=lambda x: x['avg_retrieval_time'])
    
    print(f"\nğŸ† æœ€ä½³Recall@5: {best_recall5['model_name']} ({best_recall5['recall'].get(5, 0):.1%})")
    print(f"âš¡ æœ€å¿«é€Ÿåº¦: {fastest['model_name']} ({fastest['avg_retrieval_time']*1000:.1f}ms)")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("\n" + "ğŸš€" * 40)
    logger.info("Embeddingæ¨¡å‹è¯„ä¼°å®éªŒ")
    logger.info("ğŸš€" * 40 + "\n")
    
    try:
        # 1. åŠ è½½æ•°æ®
        logger.info("ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
        queries = load_queries()
        ground_truth = load_ground_truth()
        
        logger.info(f"  âœ… åŠ è½½äº† {len(queries)} ä¸ªqueries")
        logger.info(f"  âœ… åŠ è½½äº† {len(ground_truth)} ä¸ªground truth\n")
        
        # 2. è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        all_results = []
        
        for model_id in ['M1', 'M2', 'M3', 'M4']:
            config = EMBEDDING_MODELS[model_id]
            
            try:
                result = evaluate_model(model_id, config, queries, ground_truth)
                all_results.append(result)
            except Exception as e:
                logger.error(f"âŒ {model_id} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„ç»“æœ
        if not all_results:
            logger.error("âŒ æ‰€æœ‰æ¨¡å‹è¯„ä¼°éƒ½å¤±è´¥äº†ï¼")
            return
        
        # 3. ä¿å­˜ç»“æœ
        logger.info("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
        save_results(all_results)
        
        # 4. æ‰“å°å¯¹æ¯”
        logger.info("\nğŸ“Š è¯„ä¼°ç»“æœå¯¹æ¯”ï¼š")
        print_comparison_table(all_results)
        
        logger.info("\n" + "="*80)
        logger.info("âœ… è¯„ä¼°å®Œæˆï¼")
        logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: experiments/embedding/results/")
        logger.info("="*80 + "\n")
        
    except Exception as e:
        logger.error(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()